<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - cov.info - stablehlo/dialect/TypeInference.cpp</title>
  <link rel="stylesheet" type="text/css" href="../../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../../index.html">top level</a> - <a href="index.html">stablehlo/dialect</a> - TypeInference.cpp<span style="font-size: 80%;"> (source / <a href="TypeInference.cpp.func-sort-c.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">cov.info</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">858</td>
            <td class="headerCovTableEntry">860</td>
            <td class="headerCovTableEntryHi">99.8 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2022-11-15 16:34:39</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">36</td>
            <td class="headerCovTableEntry">36</td>
            <td class="headerCovTableEntryHi">100.0 %</td>
          </tr>
          <tr><td><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : /* Copyright 2019 The TensorFlow Authors. All Rights Reserved.</a>
<a name="2"><span class="lineNum">       2 </span>            :    Copyright 2022 The StableHLO Authors.</a>
<a name="3"><span class="lineNum">       3 </span>            : </a>
<a name="4"><span class="lineNum">       4 </span>            : Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</a>
<a name="5"><span class="lineNum">       5 </span>            : you may not use this file except in compliance with the License.</a>
<a name="6"><span class="lineNum">       6 </span>            : You may obtain a copy of the License at</a>
<a name="7"><span class="lineNum">       7 </span>            : </a>
<a name="8"><span class="lineNum">       8 </span>            :     http://www.apache.org/licenses/LICENSE-2.0</a>
<a name="9"><span class="lineNum">       9 </span>            : </a>
<a name="10"><span class="lineNum">      10 </span>            : Unless required by applicable law or agreed to in writing, software</a>
<a name="11"><span class="lineNum">      11 </span>            : distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</a>
<a name="12"><span class="lineNum">      12 </span>            : WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</a>
<a name="13"><span class="lineNum">      13 </span>            : See the License for the specific language governing permissions and</a>
<a name="14"><span class="lineNum">      14 </span>            : limitations under the License.</a>
<a name="15"><span class="lineNum">      15 </span>            : ==============================================================================*/</a>
<a name="16"><span class="lineNum">      16 </span>            : </a>
<a name="17"><span class="lineNum">      17 </span>            : #include &quot;stablehlo/dialect/TypeInference.h&quot;</a>
<a name="18"><span class="lineNum">      18 </span>            : </a>
<a name="19"><span class="lineNum">      19 </span>            : #include &lt;assert.h&gt;</a>
<a name="20"><span class="lineNum">      20 </span>            : #include &lt;stddef.h&gt;</a>
<a name="21"><span class="lineNum">      21 </span>            : #include &lt;stdint.h&gt;</a>
<a name="22"><span class="lineNum">      22 </span>            : </a>
<a name="23"><span class="lineNum">      23 </span>            : #include &lt;algorithm&gt;</a>
<a name="24"><span class="lineNum">      24 </span>            : #include &lt;array&gt;</a>
<a name="25"><span class="lineNum">      25 </span>            : #include &lt;cstdint&gt;</a>
<a name="26"><span class="lineNum">      26 </span>            : #include &lt;functional&gt;</a>
<a name="27"><span class="lineNum">      27 </span>            : #include &lt;numeric&gt;</a>
<a name="28"><span class="lineNum">      28 </span>            : #include &lt;set&gt;</a>
<a name="29"><span class="lineNum">      29 </span>            : #include &lt;unordered_map&gt;</a>
<a name="30"><span class="lineNum">      30 </span>            : #include &lt;utility&gt;</a>
<a name="31"><span class="lineNum">      31 </span>            : </a>
<a name="32"><span class="lineNum">      32 </span>            : #include &quot;llvm/ADT/APInt.h&quot;</a>
<a name="33"><span class="lineNum">      33 </span>            : #include &quot;llvm/ADT/ArrayRef.h&quot;</a>
<a name="34"><span class="lineNum">      34 </span>            : #include &quot;llvm/ADT/DenseMap.h&quot;</a>
<a name="35"><span class="lineNum">      35 </span>            : #include &quot;llvm/ADT/STLExtras.h&quot;</a>
<a name="36"><span class="lineNum">      36 </span>            : #include &quot;llvm/ADT/SmallVector.h&quot;</a>
<a name="37"><span class="lineNum">      37 </span>            : #include &quot;llvm/ADT/StringExtras.h&quot;</a>
<a name="38"><span class="lineNum">      38 </span>            : #include &quot;llvm/ADT/StringRef.h&quot;</a>
<a name="39"><span class="lineNum">      39 </span>            : #include &quot;llvm/ADT/Twine.h&quot;</a>
<a name="40"><span class="lineNum">      40 </span>            : #include &quot;llvm/ADT/iterator_range.h&quot;</a>
<a name="41"><span class="lineNum">      41 </span>            : #include &quot;llvm/Support/Casting.h&quot;</a>
<a name="42"><span class="lineNum">      42 </span>            : #include &quot;llvm/Support/MathExtras.h&quot;</a>
<a name="43"><span class="lineNum">      43 </span>            : #include &quot;mlir/IR/Attributes.h&quot;</a>
<a name="44"><span class="lineNum">      44 </span>            : #include &quot;mlir/IR/BuiltinAttributes.h&quot;</a>
<a name="45"><span class="lineNum">      45 </span>            : #include &quot;mlir/IR/BuiltinTypes.h&quot;</a>
<a name="46"><span class="lineNum">      46 </span>            : #include &quot;mlir/IR/Diagnostics.h&quot;</a>
<a name="47"><span class="lineNum">      47 </span>            : #include &quot;mlir/IR/Location.h&quot;</a>
<a name="48"><span class="lineNum">      48 </span>            : #include &quot;mlir/IR/Operation.h&quot;</a>
<a name="49"><span class="lineNum">      49 </span>            : #include &quot;mlir/IR/OperationSupport.h&quot;</a>
<a name="50"><span class="lineNum">      50 </span>            : #include &quot;mlir/IR/TypeUtilities.h&quot;</a>
<a name="51"><span class="lineNum">      51 </span>            : #include &quot;mlir/IR/Types.h&quot;</a>
<a name="52"><span class="lineNum">      52 </span>            : #include &quot;mlir/IR/Value.h&quot;</a>
<a name="53"><span class="lineNum">      53 </span>            : #include &quot;mlir/Interfaces/InferTypeOpInterface.h&quot;</a>
<a name="54"><span class="lineNum">      54 </span>            : #include &quot;mlir/Support/LLVM.h&quot;</a>
<a name="55"><span class="lineNum">      55 </span>            : #include &quot;mlir/Support/LogicalResult.h&quot;</a>
<a name="56"><span class="lineNum">      56 </span>            : #include &quot;stablehlo/dialect/Base.h&quot;</a>
<a name="57"><span class="lineNum">      57 </span>            : </a>
<a name="58"><span class="lineNum">      58 </span>            : namespace mlir {</a>
<a name="59"><span class="lineNum">      59 </span>            : namespace hlo {</a>
<a name="60"><span class="lineNum">      60 </span>            : </a>
<a name="61"><span class="lineNum">      61 </span>            : //===----------------------------------------------------------------------===//</a>
<a name="62"><span class="lineNum">      62 </span>            : // Utils for shape functions.</a>
<a name="63"><span class="lineNum">      63 </span>            : //===----------------------------------------------------------------------===//</a>
<a name="64"><span class="lineNum">      64 </span>            : </a>
<a name="65"><span class="lineNum">      65 </span>            : // Return true if type1 and type2 are tensors and have the same</a>
<a name="66"><span class="lineNum">      66 </span>            : // element-type, else return false. With float element-types, ignore comparing</a>
<a name="67"><span class="lineNum">      67 </span>            : // floating-point precision if ignoreFpPrecision is True.</a>
<a name="68"><span class="lineNum">      68 </span><span class="lineCov">       1734 : bool tensorsHaveSameElType(Type type1, Type type2, bool ignoreFpPrecision) {</span></a>
<a name="69"><span class="lineNum">      69 </span><span class="lineCov">       1734 :   auto tensorTy1 = type1.dyn_cast&lt;TensorType&gt;();</span></a>
<a name="70"><span class="lineNum">      70 </span><span class="lineCov">       1734 :   auto tensorTy2 = type2.dyn_cast&lt;TensorType&gt;();</span></a>
<a name="71"><span class="lineNum">      71 </span>            : </a>
<a name="72"><span class="lineNum">      72 </span><span class="lineCov">       1734 :   if (!tensorTy1 || !tensorTy2) return false;</span></a>
<a name="73"><span class="lineNum">      73 </span>            : </a>
<a name="74"><span class="lineNum">      74 </span><span class="lineCov">       1734 :   if (ignoreFpPrecision &amp;&amp; tensorTy1.getElementType().isa&lt;FloatType&gt;() &amp;&amp;</span></a>
<a name="75"><span class="lineNum">      75 </span><span class="lineCov">        968 :       tensorTy2.getElementType().isa&lt;FloatType&gt;())</span></a>
<a name="76"><span class="lineNum">      76 </span><span class="lineCov">        958 :     return true;</span></a>
<a name="77"><span class="lineNum">      77 </span>            : </a>
<a name="78"><span class="lineNum">      78 </span><span class="lineCov">        776 :   return tensorTy1.getElementType() == tensorTy2.getElementType();</span></a>
<a name="79"><span class="lineNum">      79 </span><span class="lineCov">       1734 : }</span></a>
<a name="80"><span class="lineNum">      80 </span>            : </a>
<a name="81"><span class="lineNum">      81 </span>            : // Return true if type1 and type2 are shape-compatible and have same element</a>
<a name="82"><span class="lineNum">      82 </span>            : // type. If 'ignoreFpPrecision' is True, then allow floats with different</a>
<a name="83"><span class="lineNum">      83 </span>            : // precisions while checking element-types.</a>
<a name="84"><span class="lineNum">      84 </span><span class="lineCov">       1367 : bool compatibleShapeAndElementType(Type type1, Type type2,</span></a>
<a name="85"><span class="lineNum">      85 </span><span class="lineCov">       1367 :                                    bool ignoreFpPrecision) {</span></a>
<a name="86"><span class="lineNum">      86 </span><span class="lineCov">       1367 :   if (failed(verifyCompatibleShape(type1, type2))) return false;</span></a>
<a name="87"><span class="lineNum">      87 </span><span class="lineCov">       2724 :   return tensorsHaveSameElType(type1.cast&lt;ShapedType&gt;(),</span></a>
<a name="88"><span class="lineNum">      88 </span><span class="lineCov">       1362 :                                type2.cast&lt;ShapedType&gt;(), ignoreFpPrecision);</span></a>
<a name="89"><span class="lineNum">      89 </span><span class="lineCov">       1367 : }</span></a>
<a name="90"><span class="lineNum">      90 </span>            : </a>
<a name="91"><span class="lineNum">      91 </span>            : // Convert a 1D dense int64 attribute to a list of values.</a>
<a name="92"><span class="lineNum">      92 </span><span class="lineCov">        552 : FailureOr&lt;SmallVector&lt;int64_t&gt;&gt; convert1DAttribute(</span></a>
<a name="93"><span class="lineNum">      93 </span>            :     Optional&lt;DenseIntElementsAttr&gt; optionalAttr, Optional&lt;Location&gt; loc,</a>
<a name="94"><span class="lineNum">      94 </span><span class="lineCov">        552 :     StringRef attrName) {</span></a>
<a name="95"><span class="lineNum">      95 </span><span class="lineCov">        552 :   if (!optionalAttr.has_value()) return SmallVector&lt;int64_t&gt;{};</span></a>
<a name="96"><span class="lineNum">      96 </span>            : </a>
<a name="97"><span class="lineNum">      97 </span><span class="lineCov">        456 :   DenseIntElementsAttr attr = *optionalAttr;</span></a>
<a name="98"><span class="lineNum">      98 </span><span class="lineCov">        456 :   auto attrType = attr.getType().cast&lt;RankedTensorType&gt;();</span></a>
<a name="99"><span class="lineNum">      99 </span><span class="lineCov">        456 :   if (attrType.getRank() != 1)</span></a>
<a name="100"><span class="lineNum">     100 </span><span class="lineCov">         10 :     return emitOptionalError(loc, &quot;expects the shape of &quot;, attrName,</span></a>
<a name="101"><span class="lineNum">     101 </span>            :                              &quot; attribute to be 1-D, but got {&quot;,</a>
<a name="102"><span class="lineNum">     102 </span><span class="lineCov">          5 :                              attrType.getShape(), &quot;}.&quot;);</span></a>
<a name="103"><span class="lineNum">     103 </span><span class="lineCov">        451 :   auto values = attr.getValues&lt;int64_t&gt;();</span></a>
<a name="104"><span class="lineNum">     104 </span><span class="lineCov">        451 :   return SmallVector&lt;int64_t&gt;{values.begin(), values.end()};</span></a>
<a name="105"><span class="lineNum">     105 </span><span class="lineCov">        552 : }</span></a>
<a name="106"><span class="lineNum">     106 </span>            : </a>
<a name="107"><span class="lineNum">     107 </span>            : // Convert a Nx2 dense int64 padding attribute to a list of tuples.</a>
<a name="108"><span class="lineNum">     108 </span><span class="lineCov">        171 : FailureOr&lt;SmallVector&lt;std::pair&lt;int64_t, int64_t&gt;&gt;&gt; convertPaddingAttribute(</span></a>
<a name="109"><span class="lineNum">     109 </span><span class="lineCov">        171 :     Optional&lt;DenseIntElementsAttr&gt; optionalAttr, Optional&lt;Location&gt; loc) {</span></a>
<a name="110"><span class="lineNum">     110 </span><span class="lineCov">        171 :   if (!optionalAttr.has_value())</span></a>
<a name="111"><span class="lineNum">     111 </span><span class="lineCov">         17 :     return SmallVector&lt;std::pair&lt;int64_t, int64_t&gt;&gt;{};</span></a>
<a name="112"><span class="lineNum">     112 </span>            : </a>
<a name="113"><span class="lineNum">     113 </span><span class="lineCov">        154 :   DenseIntElementsAttr attr = *optionalAttr;</span></a>
<a name="114"><span class="lineNum">     114 </span><span class="lineCov">        154 :   auto attrType = attr.getType().cast&lt;RankedTensorType&gt;();</span></a>
<a name="115"><span class="lineNum">     115 </span><span class="lineCov">        154 :   if (attrType.getRank() != 2 || attrType.getShape()[1] != 2)</span></a>
<a name="116"><span class="lineNum">     116 </span><span class="lineCov">          2 :     return emitOptionalError(</span></a>
<a name="117"><span class="lineNum">     117 </span><span class="lineCov">          2 :         loc, &quot;expects the shape of padding-attribute to be {N, 2}, but got {&quot;,</span></a>
<a name="118"><span class="lineNum">     118 </span><span class="lineCov">          2 :         attrType.getShape(), &quot;}.&quot;);</span></a>
<a name="119"><span class="lineNum">     119 </span>            : </a>
<a name="120"><span class="lineNum">     120 </span><span class="lineCov">        152 :   auto it = attr.getValues&lt;int64_t&gt;().begin();</span></a>
<a name="121"><span class="lineNum">     121 </span><span class="lineCov">        152 :   SmallVector&lt;std::pair&lt;int64_t, int64_t&gt;&gt; out(attr.getNumElements() / 2);</span></a>
<a name="122"><span class="lineNum">     122 </span><span class="lineCov">        490 :   for (auto&amp; item : out) {</span></a>
<a name="123"><span class="lineNum">     123 </span><span class="lineCov">        338 :     int64_t first = *it;</span></a>
<a name="124"><span class="lineNum">     124 </span><span class="lineCov">        338 :     ++it;</span></a>
<a name="125"><span class="lineNum">     125 </span><span class="lineCov">        338 :     int64_t second = *it;</span></a>
<a name="126"><span class="lineNum">     126 </span><span class="lineCov">        338 :     ++it;</span></a>
<a name="127"><span class="lineNum">     127 </span><span class="lineCov">        338 :     item = {first, second};</span></a>
<a name="128"><span class="lineNum">     128 </span><span class="lineCov">        338 :   }</span></a>
<a name="129"><span class="lineNum">     129 </span><span class="lineCov">        152 :   return out;</span></a>
<a name="130"><span class="lineNum">     130 </span><span class="lineCov">        171 : }</span></a>
<a name="131"><span class="lineNum">     131 </span>            : </a>
<a name="132"><span class="lineNum">     132 </span>            : // If a window with the given bound in some dimension is dilated with the given</a>
<a name="133"><span class="lineNum">     133 </span>            : // dilation factor in that dimension, then the value returned is the bound for</a>
<a name="134"><span class="lineNum">     134 </span>            : // the array in that dimension after dilation.</a>
<a name="135"><span class="lineNum">     135 </span>            : //</a>
<a name="136"><span class="lineNum">     136 </span>            : // For a 1D array with 3 entries 1, 2, 3, a dilation factor of 2 yields a new</a>
<a name="137"><span class="lineNum">     137 </span>            : // window with values 1, x, 2, x, 3, where x indicates holes left by the</a>
<a name="138"><span class="lineNum">     138 </span>            : // dilation. So DilatedBound(3, 2) == 5.</a>
<a name="139"><span class="lineNum">     139 </span><span class="lineCov">        720 : int64_t dilatedBound(int64_t bound, int64_t dilation) {</span></a>
<a name="140"><span class="lineNum">     140 </span><span class="lineCov">        720 :   assert(bound &gt;= 0 &amp;&amp; &quot;The dimension to dialate must be &gt;= 0&quot;);</span></a>
<a name="141"><span class="lineNum">     141 </span><span class="lineCov">        720 :   if (bound == 0) return 0;</span></a>
<a name="142"><span class="lineNum">     142 </span>            : </a>
<a name="143"><span class="lineNum">     143 </span>            :   // Suppose the array has three entries 123 and the dilation factor is 4. Then</a>
<a name="144"><span class="lineNum">     144 </span>            :   // the dilated array has 9 entries 1xxx2xxx3. Here, each original entry except</a>
<a name="145"><span class="lineNum">     145 </span>            :   // the last expands into 4 entries, so that is (bound - 1) * dilation. Then we</a>
<a name="146"><span class="lineNum">     146 </span>            :   // add 1 to account for the final input element.</a>
<a name="147"><span class="lineNum">     147 </span><span class="lineCov">        720 :   return (bound - 1) * dilation + 1;</span></a>
<a name="148"><span class="lineNum">     148 </span><span class="lineCov">        720 : }</span></a>
<a name="149"><span class="lineNum">     149 </span>            : </a>
<a name="150"><span class="lineNum">     150 </span>            : // Returns the number of valid positions of a window with the given size and</a>
<a name="151"><span class="lineNum">     151 </span>            : // stride within an array with the given bound. This is the bound of an output</a>
<a name="152"><span class="lineNum">     152 </span>            : // array with one element per valid position of the window.</a>
<a name="153"><span class="lineNum">     153 </span>            : //</a>
<a name="154"><span class="lineNum">     154 </span>            : // For example, for arguments of (bound=5, window_size=2, stride=2), the</a>
<a name="155"><span class="lineNum">     155 </span>            : // returned value is 2. There are valid positions at offset 0 and offset 2,</a>
<a name="156"><span class="lineNum">     156 </span>            : // while offset 4 is not valid since the window's last entry would be at 5,</a>
<a name="157"><span class="lineNum">     157 </span>            : // which is beyond the bound of 5.</a>
<a name="158"><span class="lineNum">     158 </span><span class="lineCov">        360 : int64_t stridedBound(int64_t bound, int64_t windowSize, int64_t stride) {</span></a>
<a name="159"><span class="lineNum">     159 </span><span class="lineCov">        360 :   assert(windowSize &gt;= 0 &amp;&amp; &quot;Expected window size to be &gt;= 0&quot;);</span></a>
<a name="160"><span class="lineNum">     160 </span><span class="lineCov">        360 :   assert(bound &gt;= 0 &amp;&amp; &quot;Expected bound to be &gt;= 0&quot;);</span></a>
<a name="161"><span class="lineNum">     161 </span>            : </a>
<a name="162"><span class="lineNum">     162 </span><span class="lineCov">        360 :   if (bound == 0 || windowSize &gt; bound) return 0;</span></a>
<a name="163"><span class="lineNum">     163 </span>            : </a>
<a name="164"><span class="lineNum">     164 </span>            :   // Without considering stride, the maximum valid offset is bound -</a>
<a name="165"><span class="lineNum">     165 </span>            :   // window_size. Taking stride into account, the valid offsets then have the</a>
<a name="166"><span class="lineNum">     166 </span>            :   // form q * stride for q = 0, ..., Q such that q * stride &lt;= bound -</a>
<a name="167"><span class="lineNum">     167 </span>            :   // window_size. This implies that Q equals floor(bound - window_size /</a>
<a name="168"><span class="lineNum">     168 </span>            :   // stride). There are Q + 1 valid values of q, yielding the formula below.</a>
<a name="169"><span class="lineNum">     169 </span><span class="lineCov">        360 :   return (bound - windowSize) / stride + 1;</span></a>
<a name="170"><span class="lineNum">     170 </span><span class="lineCov">        360 : }</span></a>
<a name="171"><span class="lineNum">     171 </span>            : </a>
<a name="172"><span class="lineNum">     172 </span><span class="lineCov">         58 : LogicalResult verifyBatchNorm(Optional&lt;Location&gt; location, Value operand,</span></a>
<a name="173"><span class="lineNum">     173 </span><span class="lineCov">         58 :                               Value scale, int64_t feature_index) {</span></a>
<a name="174"><span class="lineNum">     174 </span><span class="lineCov">         58 :   auto operandType = operand.getType().cast&lt;RankedTensorType&gt;();</span></a>
<a name="175"><span class="lineNum">     175 </span><span class="lineCov">         58 :   if (feature_index &gt;= operandType.getRank())</span></a>
<a name="176"><span class="lineNum">     176 </span><span class="lineCov">          3 :     return emitOptionalError(</span></a>
<a name="177"><span class="lineNum">     177 </span><span class="lineCov">          3 :         location,</span></a>
<a name="178"><span class="lineNum">     178 </span>            :         &quot;expects feature_index to be smaller than the rank of &quot;</a>
<a name="179"><span class="lineNum">     179 </span>            :         &quot;operand type; got feature_index &quot;,</a>
<a name="180"><span class="lineNum">     180 </span><span class="lineCov">          3 :         feature_index, &quot;, and rank &quot;, operandType.getRank(), &quot;.&quot;);</span></a>
<a name="181"><span class="lineNum">     181 </span>            : </a>
<a name="182"><span class="lineNum">     182 </span><span class="lineCov">         55 :   if (feature_index &lt; 0)</span></a>
<a name="183"><span class="lineNum">     183 </span><span class="lineCov">          3 :     return emitOptionalError(location, &quot;expects feature_index to be a &quot;,</span></a>
<a name="184"><span class="lineNum">     184 </span>            :                              &quot;non-negative number, got &quot;, feature_index, &quot;.&quot;);</a>
<a name="185"><span class="lineNum">     185 </span>            : </a>
<a name="186"><span class="lineNum">     186 </span>            :   // Note: the above checks '0 &lt;= feature-index &lt; operandType.getRank()'</a>
<a name="187"><span class="lineNum">     187 </span>            :   // imply 'operand_type.getRank() &gt;= 1'.</a>
<a name="188"><span class="lineNum">     188 </span>            : </a>
<a name="189"><span class="lineNum">     189 </span><span class="lineCov">         52 :   const int64_t featureCount = operandType.getDimSize(feature_index);</span></a>
<a name="190"><span class="lineNum">     190 </span><span class="lineCov">        104 :   const int64_t scaleShape =</span></a>
<a name="191"><span class="lineNum">     191 </span><span class="lineCov">         52 :       scale.getType().cast&lt;RankedTensorType&gt;().getDimSize(0);</span></a>
<a name="192"><span class="lineNum">     192 </span>            :   // As ODS enforces `scale`, `mean`, `variance`, `offset` are AllShapesMatch,</a>
<a name="193"><span class="lineNum">     193 </span>            :   // this also infers that featureCount is aligned with them.</a>
<a name="194"><span class="lineNum">     194 </span><span class="lineCov">         52 :   if (scaleShape != featureCount)</span></a>
<a name="195"><span class="lineNum">     195 </span><span class="lineCov">          4 :     return emitOptionalError(</span></a>
<a name="196"><span class="lineNum">     196 </span><span class="lineCov">          4 :         location,</span></a>
<a name="197"><span class="lineNum">     197 </span>            :         &quot;expects the size of scale factor to be same as the &quot;</a>
<a name="198"><span class="lineNum">     198 </span>            :         &quot;feature count, but the size of scale factor is &quot;,</a>
<a name="199"><span class="lineNum">     199 </span>            :         scaleShape, &quot; and the feature count is &quot;, featureCount, &quot;.&quot;);</a>
<a name="200"><span class="lineNum">     200 </span>            : </a>
<a name="201"><span class="lineNum">     201 </span><span class="lineCov">         48 :   return success();</span></a>
<a name="202"><span class="lineNum">     202 </span><span class="lineCov">         58 : }</span></a>
<a name="203"><span class="lineNum">     203 </span>            : </a>
<a name="204"><span class="lineNum">     204 </span>            : // Verifies various properties of window-attributes (viz., stride, padding,</a>
<a name="205"><span class="lineNum">     205 </span>            : // lhs_dilation and rhs_dilation) and collects all the window-attributes for</a>
<a name="206"><span class="lineNum">     206 </span>            : // each kernel spatial dimensions.</a>
<a name="207"><span class="lineNum">     207 </span>            : FailureOr&lt;SmallVector&lt;WindowDimension&gt;&gt;</a>
<a name="208"><span class="lineNum">     208 </span><span class="lineCov">        166 : verifyWindowAttributesAndInferWindowDimensions(</span></a>
<a name="209"><span class="lineNum">     209 </span>            :     ArrayRef&lt;int64_t&gt; windowDimensions, ArrayRef&lt;int64_t&gt; windowStrides,</a>
<a name="210"><span class="lineNum">     210 </span>            :     ArrayRef&lt;std::pair&lt;int64_t, int64_t&gt;&gt; padding,</a>
<a name="211"><span class="lineNum">     211 </span>            :     ArrayRef&lt;int64_t&gt; lhsDilation, ArrayRef&lt;int64_t&gt; rhsDilation,</a>
<a name="212"><span class="lineNum">     212 </span><span class="lineCov">        166 :     Optional&lt;Location&gt; loc) {</span></a>
<a name="213"><span class="lineNum">     213 </span><span class="lineCov">        815 :   const auto verifySize = [&amp;](const size_t attrSize,</span></a>
<a name="214"><span class="lineNum">     214 </span><span class="lineCov">        649 :                               StringRef attrName) -&gt; LogicalResult {</span></a>
<a name="215"><span class="lineNum">     215 </span><span class="lineCov">        649 :     if (attrSize == 0 || attrSize == windowDimensions.size()) return success();</span></a>
<a name="216"><span class="lineNum">     216 </span><span class="lineCov">         11 :     return emitOptionalError(</span></a>
<a name="217"><span class="lineNum">     217 </span><span class="lineCov">         11 :         loc, &quot;expects &quot;, attrName,</span></a>
<a name="218"><span class="lineNum">     218 </span>            :         &quot; to have same dimension-size as size of window dimensions (&quot;,</a>
<a name="219"><span class="lineNum">     219 </span><span class="lineCov">         11 :         windowDimensions.size(), &quot;), but got: &quot;, attrSize, &quot;.&quot;);</span></a>
<a name="220"><span class="lineNum">     220 </span><span class="lineCov">        649 :   };</span></a>
<a name="221"><span class="lineNum">     221 </span>            : </a>
<a name="222"><span class="lineNum">     222 </span><span class="lineCov">        166 :   if (failed(verifySize(windowStrides.size(), &quot;window-strides&quot;)))</span></a>
<a name="223"><span class="lineNum">     223 </span><span class="lineCov">          3 :     return failure();</span></a>
<a name="224"><span class="lineNum">     224 </span><span class="lineCov">        163 :   if (failed(verifySize(lhsDilation.size(), &quot;base-dilation factors&quot;)))</span></a>
<a name="225"><span class="lineNum">     225 </span><span class="lineCov">          2 :     return failure();</span></a>
<a name="226"><span class="lineNum">     226 </span><span class="lineCov">        161 :   if (failed(verifySize(rhsDilation.size(), &quot;window-dilation factors&quot;)))</span></a>
<a name="227"><span class="lineNum">     227 </span><span class="lineCov">          2 :     return failure();</span></a>
<a name="228"><span class="lineNum">     228 </span><span class="lineCov">        159 :   if (failed(verifySize(padding.size(), &quot;padding-entries&quot;))) return failure();</span></a>
<a name="229"><span class="lineNum">     229 </span>            : </a>
<a name="230"><span class="lineNum">     230 </span><span class="lineCov">        155 :   SmallVector&lt;WindowDimension&gt; window(windowDimensions.size());</span></a>
<a name="231"><span class="lineNum">     231 </span><span class="lineCov">        519 :   for (size_t i = 0; i &lt; windowDimensions.size(); i++) {</span></a>
<a name="232"><span class="lineNum">     232 </span><span class="lineCov">        364 :     WindowDimension&amp; dim = window[i];</span></a>
<a name="233"><span class="lineNum">     233 </span>            : </a>
<a name="234"><span class="lineNum">     234 </span><span class="lineCov">        364 :     dim.size = windowDimensions[i];</span></a>
<a name="235"><span class="lineNum">     235 </span><span class="lineCov">        364 :     if (!isDynamicDimSize(dim.size) &amp;&amp; dim.size &lt;= 0)</span></a>
<a name="236"><span class="lineNum">     236 </span><span class="lineCov">          6 :       return emitOptionalError(loc,</span></a>
<a name="237"><span class="lineNum">     237 </span>            :                                &quot;expects window to have positive value for &quot;, i,</a>
<a name="238"><span class="lineNum">     238 </span><span class="lineCov">          3 :                                &quot;-th window dimension, but got &quot;, dim.size, &quot;.&quot;);</span></a>
<a name="239"><span class="lineNum">     239 </span>            : </a>
<a name="240"><span class="lineNum">     240 </span><span class="lineCov">        361 :     if (!windowStrides.empty()) dim.stride = windowStrides[i];</span></a>
<a name="241"><span class="lineNum">     241 </span><span class="lineCov">        361 :     if (dim.stride &lt;= 0)</span></a>
<a name="242"><span class="lineNum">     242 </span><span class="lineCov">          3 :       return emitOptionalError(</span></a>
<a name="243"><span class="lineNum">     243 </span><span class="lineCov">          3 :           loc, &quot;expects window to have positive stride for &quot;, i,</span></a>
<a name="244"><span class="lineNum">     244 </span><span class="lineCov">          3 :           &quot;-th window dimension, but got &quot;, dim.stride, &quot;.&quot;);</span></a>
<a name="245"><span class="lineNum">     245 </span>            : </a>
<a name="246"><span class="lineNum">     246 </span><span class="lineCov">        358 :     if (!lhsDilation.empty()) dim.baseDilation = lhsDilation[i];</span></a>
<a name="247"><span class="lineNum">     247 </span><span class="lineCov">        358 :     if (dim.baseDilation &lt;= 0)</span></a>
<a name="248"><span class="lineNum">     248 </span><span class="lineCov">          2 :       return emitOptionalError(</span></a>
<a name="249"><span class="lineNum">     249 </span><span class="lineCov">          2 :           loc, &quot;expects window to have positive base dilation factor for &quot;, i,</span></a>
<a name="250"><span class="lineNum">     250 </span><span class="lineCov">          2 :           &quot;-th window dimension, but got &quot;, dim.baseDilation, &quot;.&quot;);</span></a>
<a name="251"><span class="lineNum">     251 </span>            : </a>
<a name="252"><span class="lineNum">     252 </span><span class="lineCov">        356 :     if (!rhsDilation.empty()) dim.windowDilation = rhsDilation[i];</span></a>
<a name="253"><span class="lineNum">     253 </span><span class="lineCov">        356 :     if (dim.windowDilation &lt;= 0)</span></a>
<a name="254"><span class="lineNum">     254 </span><span class="lineCov">          2 :       return emitOptionalError(</span></a>
<a name="255"><span class="lineNum">     255 </span><span class="lineCov">          2 :           loc, &quot;expects window to have positive window dilation factor for &quot;, i,</span></a>
<a name="256"><span class="lineNum">     256 </span><span class="lineCov">          2 :           &quot;-th window dimension, but got &quot;, dim.windowDilation, &quot;.&quot;);</span></a>
<a name="257"><span class="lineNum">     257 </span>            : </a>
<a name="258"><span class="lineNum">     258 </span><span class="lineCov">        354 :     if (!padding.empty()) {</span></a>
<a name="259"><span class="lineNum">     259 </span><span class="lineCov">        294 :       dim.paddingLow = padding[i].first;</span></a>
<a name="260"><span class="lineNum">     260 </span><span class="lineCov">        294 :       dim.paddingHigh = padding[i].second;</span></a>
<a name="261"><span class="lineNum">     261 </span><span class="lineCov">        294 :     }</span></a>
<a name="262"><span class="lineNum">     262 </span><span class="lineCov">        364 :   }</span></a>
<a name="263"><span class="lineNum">     263 </span>            : </a>
<a name="264"><span class="lineNum">     264 </span><span class="lineCov">        145 :   return window;</span></a>
<a name="265"><span class="lineNum">     265 </span><span class="lineCov">        166 : }</span></a>
<a name="266"><span class="lineNum">     266 </span>            : </a>
<a name="267"><span class="lineNum">     267 </span>            : // Infer the shape of the output window.</a>
<a name="268"><span class="lineNum">     268 </span>            : //  Foreach dimension d,</a>
<a name="269"><span class="lineNum">     269 </span>            : //    output-window-shape[d] =</a>
<a name="270"><span class="lineNum">     270 </span>            : //            stridedBound(padding_low + dilatedBound(base_shape[d]) +</a>
<a name="271"><span class="lineNum">     271 </span>            : //            padding_high,</a>
<a name="272"><span class="lineNum">     272 </span>            : //                         dilatedBound(window_shape[d]))</a>
<a name="273"><span class="lineNum">     273 </span>            : //      where (padding_low, padding_high) is the padding-pair for d.</a>
<a name="274"><span class="lineNum">     274 </span><span class="lineCov">        154 : SmallVector&lt;int64_t&gt; inferWindowOutputShape(</span></a>
<a name="275"><span class="lineNum">     275 </span><span class="lineCov">        154 :     const ArrayRef&lt;int64_t&gt; baseShape, const ArrayRef&lt;WindowDimension&gt; window) {</span></a>
<a name="276"><span class="lineNum">     276 </span><span class="lineCov">        154 :   assert(baseShape.size() == window.size() &amp;&amp;</span></a>
<a name="277"><span class="lineNum">     277 </span>            :          &quot;Size of window dimensions must match the size of base shape.&quot;);</a>
<a name="278"><span class="lineNum">     278 </span>            : </a>
<a name="279"><span class="lineNum">     279 </span><span class="lineCov">        154 :   SmallVector&lt;int64_t&gt; outputDimensions(window.size());</span></a>
<a name="280"><span class="lineNum">     280 </span><span class="lineCov">        518 :   for (int64_t i = 0; i &lt; static_cast&lt;int64_t&gt;(window.size()); ++i) {</span></a>
<a name="281"><span class="lineNum">     281 </span><span class="lineCov">        364 :     if (isDynamicDimSize(baseShape[i]) || isDynamicDimSize(window[i].size)) {</span></a>
<a name="282"><span class="lineNum">     282 </span><span class="lineCov">          4 :       outputDimensions[i] = ShapedType::kDynamicSize;</span></a>
<a name="283"><span class="lineNum">     283 </span><span class="lineCov">          4 :     } else {</span></a>
<a name="284"><span class="lineNum">     284 </span><span class="lineCov">        360 :       const auto&amp; dim = window[i];</span></a>
<a name="285"><span class="lineNum">     285 </span>            : </a>
<a name="286"><span class="lineNum">     286 </span><span class="lineCov">        360 :       const int64_t dilatedBase = dilatedBound(baseShape[i], dim.baseDilation);</span></a>
<a name="287"><span class="lineNum">     287 </span><span class="lineCov">        720 :       const int64_t paddedDilatedBase =</span></a>
<a name="288"><span class="lineNum">     288 </span><span class="lineCov">        360 :           dim.paddingLow + dilatedBase + dim.paddingHigh;</span></a>
<a name="289"><span class="lineNum">     289 </span><span class="lineCov">        360 :       const int64_t dilatedWindow = dilatedBound(dim.size, dim.windowDilation);</span></a>
<a name="290"><span class="lineNum">     290 </span>            : </a>
<a name="291"><span class="lineNum">     291 </span><span class="lineCov">        360 :       outputDimensions[i] =</span></a>
<a name="292"><span class="lineNum">     292 </span><span class="lineCov">        360 :           stridedBound(paddedDilatedBase, dilatedWindow, dim.stride);</span></a>
<a name="293"><span class="lineNum">     293 </span><span class="lineCov">        360 :     }</span></a>
<a name="294"><span class="lineNum">     294 </span><span class="lineCov">        364 :   }</span></a>
<a name="295"><span class="lineNum">     295 </span>            : </a>
<a name="296"><span class="lineNum">     296 </span><span class="lineCov">        154 :   return outputDimensions;</span></a>
<a name="297"><span class="lineNum">     297 </span><span class="lineCov">        154 : }</span></a>
<a name="298"><span class="lineNum">     298 </span>            : </a>
<a name="299"><span class="lineNum">     299 </span><span class="lineCov">         72 : unsigned potentiallyComplexBitwidth(Type type) {</span></a>
<a name="300"><span class="lineNum">     300 </span><span class="lineCov">         72 :   auto complexTy = type.dyn_cast&lt;ComplexType&gt;();</span></a>
<a name="301"><span class="lineNum">     301 </span><span class="lineCov">         72 :   return complexTy ? 2 * complexTy.getElementType().getIntOrFloatBitWidth()</span></a>
<a name="302"><span class="lineNum">     302 </span><span class="lineCov">         68 :                    : type.getIntOrFloatBitWidth();</span></a>
<a name="303"><span class="lineNum">     303 </span><span class="lineCov">         72 : }</span></a>
<a name="304"><span class="lineNum">     304 </span>            : </a>
<a name="305"><span class="lineNum">     305 </span><span class="lineCov">        325 : LogicalResult verifyReducerShape(</span></a>
<a name="306"><span class="lineNum">     306 </span>            :     Optional&lt;Location&gt; loc, Block&amp; block, ArrayRef&lt;TensorType&gt; inputArgTypes,</a>
<a name="307"><span class="lineNum">     307 </span>            :     ArrayRef&lt;TensorType&gt; initValueTypes, int64_t numInputs,</a>
<a name="308"><span class="lineNum">     308 </span>            :     ArrayRef&lt;int64_t&gt; allowedDimensions, bool allInputsUnranked,</a>
<a name="309"><span class="lineNum">     309 </span><span class="lineCov">        325 :     SmallVectorImpl&lt;TensorType&gt;&amp; accumulatorSubShapes) {</span></a>
<a name="310"><span class="lineNum">     310 </span>            :   // Check that the number of reduction-region arguments matches with that of</a>
<a name="311"><span class="lineNum">     311 </span>            :   // reduce-op's arguments.</a>
<a name="312"><span class="lineNum">     312 </span><span class="lineCov">        325 :   if (static_cast&lt;int64_t&gt;(block.getArguments().size()) != numInputs * 2)</span></a>
<a name="313"><span class="lineNum">     313 </span><span class="lineCov">         10 :     return emitOptionalError(loc, &quot;Reduction-region must take &quot;, numInputs * 2,</span></a>
<a name="314"><span class="lineNum">     314 </span>            :                              &quot; parameters, but takes &quot;,</a>
<a name="315"><span class="lineNum">     315 </span><span class="lineCov">          5 :                              block.getArguments().size(), &quot; parameter(s)&quot;);</span></a>
<a name="316"><span class="lineNum">     316 </span>            : </a>
<a name="317"><span class="lineNum">     317 </span>            :   // Check if the reduction-region produces non-zero outputs.</a>
<a name="318"><span class="lineNum">     318 </span><span class="lineCov">        320 :   if (block.getTerminator()-&gt;getOperands().empty())</span></a>
<a name="319"><span class="lineNum">     319 </span><span class="lineCov">          5 :     return emitOptionalError(</span></a>
<a name="320"><span class="lineNum">     320 </span><span class="lineCov">          5 :         loc, &quot;The reduction-region expected to return some value(s)&quot;);</span></a>
<a name="321"><span class="lineNum">     321 </span>            : </a>
<a name="322"><span class="lineNum">     322 </span>            :   // Check that the reduction-region returns list- of tensors.</a>
<a name="323"><span class="lineNum">     323 </span>            :   // The number of result-tensors must match the `numInputs`.</a>
<a name="324"><span class="lineNum">     324 </span><span class="lineCov">        630 :   if (static_cast&lt;int64_t&gt;(block.getTerminator()-&gt;getOperands().size()) !=</span></a>
<a name="325"><span class="lineNum">     325 </span><span class="lineCov">        315 :       numInputs)</span></a>
<a name="326"><span class="lineNum">     326 </span><span class="lineCov">          8 :     return emitOptionalError(loc, &quot;Reduction-region here must produce &quot;,</span></a>
<a name="327"><span class="lineNum">     327 </span>            :                              numInputs, &quot; tensors, but produces &quot;,</a>
<a name="328"><span class="lineNum">     328 </span><span class="lineCov">          4 :                              block.getTerminator()-&gt;getOperands().size(),</span></a>
<a name="329"><span class="lineNum">     329 </span>            :                              &quot; instead&quot;);</a>
<a name="330"><span class="lineNum">     330 </span>            : </a>
<a name="331"><span class="lineNum">     331 </span><span class="lineCov">        711 :   for (Value retOperand : block.getTerminator()-&gt;getOperands()) {</span></a>
<a name="332"><span class="lineNum">     332 </span><span class="lineCov">        400 :     auto tensorTy = retOperand.getType().dyn_cast&lt;TensorType&gt;();</span></a>
<a name="333"><span class="lineNum">     333 </span><span class="lineCov">        400 :     if (!tensorTy)</span></a>
<a name="334"><span class="lineNum">     334 </span><span class="lineCov">         14 :       return emitOptionalError(loc,</span></a>
<a name="335"><span class="lineNum">     335 </span>            :                                &quot;Reduction-region here must produce &quot;</a>
<a name="336"><span class="lineNum">     336 </span>            :                                &quot;tensor-typed result(s), but &quot;</a>
<a name="337"><span class="lineNum">     337 </span>            :                                &quot;produces &quot;,</a>
<a name="338"><span class="lineNum">     338 </span><span class="lineCov">          7 :                                retOperand.getType(), &quot; instead&quot;);</span></a>
<a name="339"><span class="lineNum">     339 </span>            : </a>
<a name="340"><span class="lineNum">     340 </span><span class="lineCov">        393 :     accumulatorSubShapes.push_back(tensorTy);</span></a>
<a name="341"><span class="lineNum">     341 </span><span class="lineCov">        400 :   }</span></a>
<a name="342"><span class="lineNum">     342 </span>            : </a>
<a name="343"><span class="lineNum">     343 </span>            :   // Consider typical reduce-* op syntax:</a>
<a name="344"><span class="lineNum">     344 </span>            :   //</a>
<a name="345"><span class="lineNum">     345 </span>            :   //      op(I(i), V(j)):</a>
<a name="346"><span class="lineNum">     346 </span>            :   //       block(BI(i), BV(j)):</a>
<a name="347"><span class="lineNum">     347 </span>            :   //         ... some computation ...</a>
<a name="348"><span class="lineNum">     348 </span>            :   //         return(R(i))</a>
<a name="349"><span class="lineNum">     349 </span>            :   //</a>
<a name="350"><span class="lineNum">     350 </span>            :   // where</a>
<a name="351"><span class="lineNum">     351 </span>            :   //  I(i)  : i-th input of op</a>
<a name="352"><span class="lineNum">     352 </span>            :   //  V(j)  : j-th init-value of op</a>
<a name="353"><span class="lineNum">     353 </span>            :   //  BI(i) : i-th input of reducer-function</a>
<a name="354"><span class="lineNum">     354 </span>            :   //  BV(j) : j-th init-value of reducer-function</a>
<a name="355"><span class="lineNum">     355 </span>            :   //  R(i)  : i-th return-type</a>
<a name="356"><span class="lineNum">     356 </span>            :   //</a>
<a name="357"><span class="lineNum">     357 </span>            :   //  Note that: |I(i)| == |V(j)| == |BI(i)| == |BV(j)| == |R(i)|</a>
<a name="358"><span class="lineNum">     358 </span>            :   //</a>
<a name="359"><span class="lineNum">     359 </span>            :   //  Here are the type-constraints among V(j), BI(i), BV(j), and R(i).</a>
<a name="360"><span class="lineNum">     360 </span>            :   //    C1 : Check that BI(i) and R(i) have same shape and element-type.</a>
<a name="361"><span class="lineNum">     361 </span>            :   //    C2 : Check that BV(j) and R(i) have same shape and element-type.</a>
<a name="362"><span class="lineNum">     362 </span>            :   //    C3 : Check that V(j) and R(i) have same shape and element-type.</a>
<a name="363"><span class="lineNum">     363 </span>            :   //</a>
<a name="364"><span class="lineNum">     364 </span>            :   //  From C1, C2, and C3, we can infer that V(j), BI(i), BV(j), and R(i) all</a>
<a name="365"><span class="lineNum">     365 </span>            :   //  have compatible shapes and element-types.</a>
<a name="366"><span class="lineNum">     366 </span>            :   //  The next check, C4, adds constraints on how the type if I(i) is related</a>
<a name="367"><span class="lineNum">     367 </span>            :   //  to any_of(V(j), BI(i), BV(j), and R(i)), say BV(j);</a>
<a name="368"><span class="lineNum">     368 </span>            :   //</a>
<a name="369"><span class="lineNum">     369 </span>            :   //  C4.1 : Check that I(i) and BV(j) have same element-type.</a>
<a name="370"><span class="lineNum">     370 </span>            :   //  C4.2 : Check that shape of BV(j) is a 'sub-sequence' of</a>
<a name="371"><span class="lineNum">     371 </span>            :   //         'allowedDimensions'. 'allowedDimensions' is a list of dimensions</a>
<a name="372"><span class="lineNum">     372 </span>            :   //         which any of BI(i), BV(j), and R(i) is allowed to have.</a>
<a name="373"><span class="lineNum">     373 </span><span class="lineCov">        690 :   for (int64_t inputIdx = 0; inputIdx &lt; numInputs; ++inputIdx) {</span></a>
<a name="374"><span class="lineNum">     374 </span>            :     // Check C1.</a>
<a name="375"><span class="lineNum">     375 </span><span class="lineCov">        772 :     if (!compatibleShapeAndElementType(accumulatorSubShapes[inputIdx],</span></a>
<a name="376"><span class="lineNum">     376 </span><span class="lineCov">        386 :                                        block.getArgument(inputIdx).getType()))</span></a>
<a name="377"><span class="lineNum">     377 </span><span class="lineCov">          6 :       return emitOptionalError(</span></a>
<a name="378"><span class="lineNum">     378 </span><span class="lineCov">          6 :           loc, &quot;The type of reduction-region's parameter at index &quot;, inputIdx,</span></a>
<a name="379"><span class="lineNum">     379 </span>            :           &quot; is different than the corresponding result type: &quot;,</a>
<a name="380"><span class="lineNum">     380 </span><span class="lineCov">          6 :           block.getArgument(inputIdx).getType(), &quot; vs &quot;,</span></a>
<a name="381"><span class="lineNum">     381 </span><span class="lineCov">          6 :           accumulatorSubShapes[inputIdx]);</span></a>
<a name="382"><span class="lineNum">     382 </span>            : </a>
<a name="383"><span class="lineNum">     383 </span>            :     // Check C2.</a>
<a name="384"><span class="lineNum">     384 </span><span class="lineCov">        380 :     if (!compatibleShapeAndElementType(</span></a>
<a name="385"><span class="lineNum">     385 </span><span class="lineCov">        380 :             accumulatorSubShapes[inputIdx],</span></a>
<a name="386"><span class="lineNum">     386 </span><span class="lineCov">        380 :             block.getArgument(numInputs + inputIdx).getType(),</span></a>
<a name="387"><span class="lineNum">     387 </span>            :             /*ignoreFpPrecision=*/true))</a>
<a name="388"><span class="lineNum">     388 </span><span class="lineCov">          4 :       return emitOptionalError(</span></a>
<a name="389"><span class="lineNum">     389 </span><span class="lineCov">          4 :           loc, &quot;The type of reduction-region's parameter at index &quot;,</span></a>
<a name="390"><span class="lineNum">     390 </span><span class="lineCov">          4 :           numInputs + inputIdx,</span></a>
<a name="391"><span class="lineNum">     391 </span>            :           &quot; is different than the corresponding result type: &quot;,</a>
<a name="392"><span class="lineNum">     392 </span><span class="lineCov">          4 :           block.getArgument(numInputs + inputIdx).getType(), &quot; vs &quot;,</span></a>
<a name="393"><span class="lineNum">     393 </span><span class="lineCov">          4 :           accumulatorSubShapes[inputIdx]);</span></a>
<a name="394"><span class="lineNum">     394 </span>            : </a>
<a name="395"><span class="lineNum">     395 </span>            :     // Check C3.</a>
<a name="396"><span class="lineNum">     396 </span><span class="lineCov">        752 :     if (!compatibleShapeAndElementType(accumulatorSubShapes[inputIdx],</span></a>
<a name="397"><span class="lineNum">     397 </span><span class="lineCov">        376 :                                        initValueTypes[inputIdx],</span></a>
<a name="398"><span class="lineNum">     398 </span>            :                                        /*ignoreFpPrecision=*/true))</a>
<a name="399"><span class="lineNum">     399 </span><span class="lineCov">          4 :       return emitOptionalError(</span></a>
<a name="400"><span class="lineNum">     400 </span><span class="lineCov">          4 :           loc, &quot;The type of reduction-region's result type at index &quot;, inputIdx,</span></a>
<a name="401"><span class="lineNum">     401 </span>            :           &quot; differs from the op's corresponding init-value type: &quot;,</a>
<a name="402"><span class="lineNum">     402 </span><span class="lineCov">          4 :           accumulatorSubShapes[inputIdx], &quot; vs &quot;, initValueTypes[inputIdx]);</span></a>
<a name="403"><span class="lineNum">     403 </span>            : </a>
<a name="404"><span class="lineNum">     404 </span>            :     // Check C4.1.</a>
<a name="405"><span class="lineNum">     405 </span><span class="lineCov">        372 :     if (!tensorsHaveSameElType(</span></a>
<a name="406"><span class="lineNum">     406 </span><span class="lineCov">        372 :             inputArgTypes[inputIdx],</span></a>
<a name="407"><span class="lineNum">     407 </span><span class="lineCov">        372 :             block.getArgument(numInputs + inputIdx).getType(), true))</span></a>
<a name="408"><span class="lineNum">     408 </span><span class="lineCov">          4 :       return emitOptionalError(</span></a>
<a name="409"><span class="lineNum">     409 </span><span class="lineCov">          4 :           loc, &quot;The element-type of reduction-region's argument at index &quot;,</span></a>
<a name="410"><span class="lineNum">     410 </span><span class="lineCov">          4 :           numInputs + inputIdx, &quot; is expected to be &quot;,</span></a>
<a name="411"><span class="lineNum">     411 </span><span class="lineCov">          4 :           inputArgTypes[inputIdx].getElementType(), &quot;, but got &quot;,</span></a>
<a name="412"><span class="lineNum">     412 </span><span class="lineCov">          4 :           block.getArgument(numInputs + inputIdx).getType(), &quot; as its type.&quot;);</span></a>
<a name="413"><span class="lineNum">     413 </span>            : </a>
<a name="414"><span class="lineNum">     414 </span>            :     // Check C4.2.</a>
<a name="415"><span class="lineNum">     415 </span><span class="lineCov">        368 :     Type blockArgType = block.getArgument(numInputs + inputIdx).getType();</span></a>
<a name="416"><span class="lineNum">     416 </span><span class="lineCov">        368 :     auto blockArgTensorTy = blockArgType.cast&lt;TensorType&gt;();</span></a>
<a name="417"><span class="lineNum">     417 </span>            : </a>
<a name="418"><span class="lineNum">     418 </span><span class="lineCov">        368 :     if (allInputsUnranked || !blockArgTensorTy.hasRank()) return success();</span></a>
<a name="419"><span class="lineNum">     419 </span>            : </a>
<a name="420"><span class="lineNum">     420 </span><span class="lineCov">        355 :     auto argShape = blockArgTensorTy.getShape();</span></a>
<a name="421"><span class="lineNum">     421 </span><span class="lineCov">        355 :     if (argShape.size() &gt; allowedDimensions.size())</span></a>
<a name="422"><span class="lineNum">     422 </span><span class="lineCov">          3 :       return emitOptionalError(</span></a>
<a name="423"><span class="lineNum">     423 </span><span class="lineCov">          3 :           loc, &quot;The rank of reduction-region's argument at index &quot;,</span></a>
<a name="424"><span class="lineNum">     424 </span><span class="lineCov">          3 :           numInputs + inputIdx,</span></a>
<a name="425"><span class="lineNum">     425 </span><span class="lineCov">          3 :           &quot; is expected to be &lt;= &quot;, allowedDimensions.size(), &quot;, got &quot;,</span></a>
<a name="426"><span class="lineNum">     426 </span><span class="lineCov">          3 :           argShape.size());</span></a>
<a name="427"><span class="lineNum">     427 </span>            : </a>
<a name="428"><span class="lineNum">     428 </span><span class="lineCov">        352 :     int64_t argShapeIdx = 0;</span></a>
<a name="429"><span class="lineNum">     429 </span><span class="lineCov">        766 :     for (int64_t outputShapeIdx = 0;</span></a>
<a name="430"><span class="lineNum">     430 </span><span class="lineCov">        383 :          outputShapeIdx &lt; static_cast&lt;int64_t&gt;(allowedDimensions.size()) &amp;&amp;</span></a>
<a name="431"><span class="lineNum">     431 </span><span class="lineCov">        238 :          argShapeIdx &lt; static_cast&lt;int64_t&gt;(argShape.size());</span></a>
<a name="432"><span class="lineNum">     432 </span><span class="lineCov">         31 :          outputShapeIdx++)</span></a>
<a name="433"><span class="lineNum">     433 </span><span class="lineCov">         31 :       if (allowedDimensions[outputShapeIdx] == argShape[argShapeIdx] ||</span></a>
<a name="434"><span class="lineNum">     434 </span><span class="lineCov">         36 :           argShape[argShapeIdx] == ShapedType::kDynamicSize)</span></a>
<a name="435"><span class="lineNum">     435 </span><span class="lineCov">         26 :         argShapeIdx++;</span></a>
<a name="436"><span class="lineNum">     436 </span>            : </a>
<a name="437"><span class="lineNum">     437 </span><span class="lineCov">        352 :     if (argShapeIdx != static_cast&lt;int64_t&gt;(argShape.size()))</span></a>
<a name="438"><span class="lineNum">     438 </span><span class="lineCov">          2 :       return emitOptionalError(</span></a>
<a name="439"><span class="lineNum">     439 </span><span class="lineCov">          2 :           loc, &quot;The shape of reduction-region's argument at index &quot;,</span></a>
<a name="440"><span class="lineNum">     440 </span><span class="lineCov">          2 :           numInputs + inputIdx,</span></a>
<a name="441"><span class="lineNum">     441 </span>            :           &quot; is not compatible with that of reduce-op's input-parameter &quot;</a>
<a name="442"><span class="lineNum">     442 </span>            :           &quot;at index &quot;,</a>
<a name="443"><span class="lineNum">     443 </span>            :           inputIdx);</a>
<a name="444"><span class="lineNum">     444 </span><span class="lineCov">        368 :   }</span></a>
<a name="445"><span class="lineNum">     445 </span>            : </a>
<a name="446"><span class="lineNum">     446 </span><span class="lineCov">        268 :   return success();</span></a>
<a name="447"><span class="lineNum">     447 </span><span class="lineCov">        325 : }</span></a>
<a name="448"><span class="lineNum">     448 </span>            : </a>
<a name="449"><span class="lineNum">     449 </span>            : //===----------------------------------------------------------------------===//</a>
<a name="450"><span class="lineNum">     450 </span>            : // Shape functions for ops.</a>
<a name="451"><span class="lineNum">     451 </span>            : //===----------------------------------------------------------------------===//</a>
<a name="452"><span class="lineNum">     452 </span>            : </a>
<a name="453"><span class="lineNum">     453 </span><span class="lineCov">         25 : LogicalResult inferBatchNormGradOp(</span></a>
<a name="454"><span class="lineNum">     454 </span>            :     Optional&lt;Location&gt; location, Value operand, Value scale,</a>
<a name="455"><span class="lineNum">     455 </span>            :     uint64_t featureIndex,</a>
<a name="456"><span class="lineNum">     456 </span><span class="lineCov">         25 :     SmallVectorImpl&lt;ShapedTypeComponents&gt;&amp; inferredReturnShapes) {</span></a>
<a name="457"><span class="lineNum">     457 </span><span class="lineCov">         25 :   if (failed(verifyBatchNorm(location, operand, scale, featureIndex)))</span></a>
<a name="458"><span class="lineNum">     458 </span><span class="lineCov">          4 :     return failure();</span></a>
<a name="459"><span class="lineNum">     459 </span><span class="lineCov">         21 :   auto operandType = operand.getType().cast&lt;RankedTensorType&gt;();</span></a>
<a name="460"><span class="lineNum">     460 </span><span class="lineCov">         21 :   inferredReturnShapes.emplace_back(operandType.cast&lt;ShapedType&gt;());</span></a>
<a name="461"><span class="lineNum">     461 </span>            : </a>
<a name="462"><span class="lineNum">     462 </span><span class="lineCov">         21 :   const int64_t featureCount = operandType.getDimSize(featureIndex);</span></a>
<a name="463"><span class="lineNum">     463 </span><span class="lineCov">         21 :   SmallVector&lt;int64_t&gt; featureShape{featureCount};</span></a>
<a name="464"><span class="lineNum">     464 </span><span class="lineCov">         21 :   inferredReturnShapes.emplace_back(featureShape, operandType.getElementType());</span></a>
<a name="465"><span class="lineNum">     465 </span><span class="lineCov">         21 :   inferredReturnShapes.emplace_back(featureShape, operandType.getElementType());</span></a>
<a name="466"><span class="lineNum">     466 </span><span class="lineCov">         21 :   return success();</span></a>
<a name="467"><span class="lineNum">     467 </span><span class="lineCov">         25 : }</span></a>
<a name="468"><span class="lineNum">     468 </span>            : </a>
<a name="469"><span class="lineNum">     469 </span><span class="lineCov">          9 : LogicalResult inferBatchNormInferenceOp(</span></a>
<a name="470"><span class="lineNum">     470 </span>            :     Optional&lt;Location&gt; location, Value operand, Value scale,</a>
<a name="471"><span class="lineNum">     471 </span>            :     uint64_t featureIndex,</a>
<a name="472"><span class="lineNum">     472 </span><span class="lineCov">          9 :     SmallVectorImpl&lt;ShapedTypeComponents&gt;&amp; inferredReturnShapes) {</span></a>
<a name="473"><span class="lineNum">     473 </span><span class="lineCov">          9 :   if (failed(verifyBatchNorm(location, operand, scale, featureIndex)))</span></a>
<a name="474"><span class="lineNum">     474 </span><span class="lineCov">          3 :     return failure();</span></a>
<a name="475"><span class="lineNum">     475 </span><span class="lineCov">          6 :   auto operandType = operand.getType().cast&lt;RankedTensorType&gt;();</span></a>
<a name="476"><span class="lineNum">     476 </span><span class="lineCov">          6 :   inferredReturnShapes.emplace_back(operandType.cast&lt;ShapedType&gt;());</span></a>
<a name="477"><span class="lineNum">     477 </span><span class="lineCov">          6 :   return success();</span></a>
<a name="478"><span class="lineNum">     478 </span><span class="lineCov">          9 : }</span></a>
<a name="479"><span class="lineNum">     479 </span>            : </a>
<a name="480"><span class="lineNum">     480 </span><span class="lineCov">         24 : LogicalResult inferBatchNormTrainingOp(</span></a>
<a name="481"><span class="lineNum">     481 </span>            :     Optional&lt;Location&gt; location, Value operand, Value scale,</a>
<a name="482"><span class="lineNum">     482 </span>            :     uint64_t featureIndex,</a>
<a name="483"><span class="lineNum">     483 </span><span class="lineCov">         24 :     SmallVectorImpl&lt;ShapedTypeComponents&gt;&amp; inferredReturnShapes) {</span></a>
<a name="484"><span class="lineNum">     484 </span><span class="lineCov">         24 :   if (failed(verifyBatchNorm(location, operand, scale, featureIndex)))</span></a>
<a name="485"><span class="lineNum">     485 </span><span class="lineCov">          3 :     return failure();</span></a>
<a name="486"><span class="lineNum">     486 </span><span class="lineCov">         21 :   auto operandType = operand.getType().cast&lt;RankedTensorType&gt;();</span></a>
<a name="487"><span class="lineNum">     487 </span><span class="lineCov">         21 :   inferredReturnShapes.emplace_back(operandType.cast&lt;ShapedType&gt;());</span></a>
<a name="488"><span class="lineNum">     488 </span>            : </a>
<a name="489"><span class="lineNum">     489 </span><span class="lineCov">         21 :   const int64_t featureCount = operandType.getDimSize(featureIndex);</span></a>
<a name="490"><span class="lineNum">     490 </span><span class="lineCov">         21 :   SmallVector&lt;int64_t&gt; featureShape{featureCount};</span></a>
<a name="491"><span class="lineNum">     491 </span><span class="lineCov">         21 :   inferredReturnShapes.emplace_back(featureShape, operandType.getElementType());</span></a>
<a name="492"><span class="lineNum">     492 </span><span class="lineCov">         21 :   inferredReturnShapes.emplace_back(featureShape, operandType.getElementType());</span></a>
<a name="493"><span class="lineNum">     493 </span><span class="lineCov">         21 :   return success();</span></a>
<a name="494"><span class="lineNum">     494 </span><span class="lineCov">         24 : }</span></a>
<a name="495"><span class="lineNum">     495 </span>            : </a>
<a name="496"><span class="lineNum">     496 </span>            : // Used by IfOp and CaseOp</a>
<a name="497"><span class="lineNum">     497 </span><span class="lineCov">         39 : LogicalResult inferConditionalOp(Optional&lt;Location&gt; location,</span></a>
<a name="498"><span class="lineNum">     498 </span>            :                                  RegionRange branches,</a>
<a name="499"><span class="lineNum">     499 </span><span class="lineCov">         39 :                                  SmallVectorImpl&lt;Type&gt;&amp; inferredReturnTypes) {</span></a>
<a name="500"><span class="lineNum">     500 </span><span class="lineCov">         39 :   if (branches.empty())</span></a>
<a name="501"><span class="lineNum">     501 </span><span class="lineCov">          1 :     return emitOptionalError(location, &quot;expect at least one branch&quot;);</span></a>
<a name="502"><span class="lineNum">     502 </span>            : </a>
<a name="503"><span class="lineNum">     503 </span><span class="lineCov">         38 :   ValueTypeRange&lt;OperandRange&gt; branch0ResultTypes =</span></a>
<a name="504"><span class="lineNum">     504 </span><span class="lineCov">         38 :       branches[0]-&gt;front().getTerminator()-&gt;getOperandTypes();</span></a>
<a name="505"><span class="lineNum">     505 </span><span class="lineCov">        111 :   for (unsigned i = 0; i &lt; branches.size(); ++i) {</span></a>
<a name="506"><span class="lineNum">     506 </span><span class="lineCov">         73 :     Twine branchName = &quot;branch &quot; + Twine(i);</span></a>
<a name="507"><span class="lineNum">     507 </span><span class="lineCov">         73 :     Region* region = branches[i];</span></a>
<a name="508"><span class="lineNum">     508 </span><span class="lineCov">         73 :     if (region-&gt;getNumArguments() != 0)</span></a>
<a name="509"><span class="lineNum">     509 </span><span class="lineCov">          6 :       return emitOptionalError(location, branchName,</span></a>
<a name="510"><span class="lineNum">     510 </span>            :                                &quot; must have 0 arguments, but found &quot;,</a>
<a name="511"><span class="lineNum">     511 </span><span class="lineCov">          3 :                                region-&gt;getNumArguments());</span></a>
<a name="512"><span class="lineNum">     512 </span>            : </a>
<a name="513"><span class="lineNum">     513 </span><span class="lineCov">         70 :     auto branchResultTypes = region-&gt;front().getTerminator()-&gt;getOperandTypes();</span></a>
<a name="514"><span class="lineNum">     514 </span><span class="lineCov">        140 :     if (!hlo::isCompatibleForHloTypeInference(branch0ResultTypes,</span></a>
<a name="515"><span class="lineNum">     515 </span><span class="lineCov">         70 :                                               branchResultTypes))</span></a>
<a name="516"><span class="lineNum">     516 </span><span class="lineCov">          3 :       return emitOptionalError(location, &quot;branch 0 and &quot;, branchName,</span></a>
<a name="517"><span class="lineNum">     517 </span>            :                                &quot; have mismatched return types: &quot;,</a>
<a name="518"><span class="lineNum">     518 </span>            :                                branch0ResultTypes, &quot; vs &quot;, branchResultTypes);</a>
<a name="519"><span class="lineNum">     519 </span><span class="lineCov">         73 :   }</span></a>
<a name="520"><span class="lineNum">     520 </span><span class="lineCov">         64 :   for (auto resultType : branch0ResultTypes)</span></a>
<a name="521"><span class="lineNum">     521 </span><span class="lineCov">         32 :     inferredReturnTypes.push_back(resultType);</span></a>
<a name="522"><span class="lineNum">     522 </span><span class="lineCov">         32 :   return success();</span></a>
<a name="523"><span class="lineNum">     523 </span><span class="lineCov">         39 : }</span></a>
<a name="524"><span class="lineNum">     524 </span>            : </a>
<a name="525"><span class="lineNum">     525 </span><span class="lineCov">         19 : LogicalResult inferCaseOp(Optional&lt;Location&gt; location, RegionRange branches,</span></a>
<a name="526"><span class="lineNum">     526 </span><span class="lineCov">         19 :                           SmallVectorImpl&lt;Type&gt;&amp; inferredReturnTypes) {</span></a>
<a name="527"><span class="lineNum">     527 </span><span class="lineCov">         19 :   return inferConditionalOp(location, branches, inferredReturnTypes);</span></a>
<a name="528"><span class="lineNum">     528 </span>            : }</a>
<a name="529"><span class="lineNum">     529 </span>            : </a>
<a name="530"><span class="lineNum">     530 </span><span class="lineCov">         79 : LogicalResult inferConcatenateOp(Optional&lt;Location&gt; location, ValueRange inputs,</span></a>
<a name="531"><span class="lineNum">     531 </span>            :                                  int64_t dimension,</a>
<a name="532"><span class="lineNum">     532 </span><span class="lineCov">         79 :                                  SmallVectorImpl&lt;Type&gt;&amp; inferredReturnTypes) {</span></a>
<a name="533"><span class="lineNum">     533 </span><span class="lineCov">         79 :   if (dimension &lt; 0)</span></a>
<a name="534"><span class="lineNum">     534 </span><span class="lineCov">          2 :     return emitOptionalError(location, &quot;dimension &quot;, dimension, &quot; is negative&quot;);</span></a>
<a name="535"><span class="lineNum">     535 </span><span class="lineCov">         77 :   RankedTensorType firstRankedType;</span></a>
<a name="536"><span class="lineNum">     536 </span><span class="lineCov">         77 :   int firstRankedIndex = -1;</span></a>
<a name="537"><span class="lineNum">     537 </span><span class="lineCov">        242 :   for (uint64_t i = 0; i &lt; inputs.size(); i++) {</span></a>
<a name="538"><span class="lineNum">     538 </span><span class="lineCov">        165 :     auto secondType = inputs[i].getType().dyn_cast&lt;ShapedType&gt;();</span></a>
<a name="539"><span class="lineNum">     539 </span><span class="lineCov">        165 :     if (!secondType.hasRank()) continue;</span></a>
<a name="540"><span class="lineNum">     540 </span>            : </a>
<a name="541"><span class="lineNum">     541 </span><span class="lineCov">        153 :     if (!firstRankedType) {</span></a>
<a name="542"><span class="lineNum">     542 </span><span class="lineCov">         77 :       firstRankedType = secondType.cast&lt;RankedTensorType&gt;();</span></a>
<a name="543"><span class="lineNum">     543 </span><span class="lineCov">         77 :       firstRankedIndex = i;</span></a>
<a name="544"><span class="lineNum">     544 </span><span class="lineCov">         77 :       if (firstRankedType.getRank() == 0)</span></a>
<a name="545"><span class="lineNum">     545 </span><span class="lineCov">          1 :         return emitOptionalError(location,</span></a>
<a name="546"><span class="lineNum">     546 </span>            :                                  &quot;rank-0 values cannot be concatenated&quot;);</a>
<a name="547"><span class="lineNum">     547 </span><span class="lineCov">         76 :       if (dimension &gt;= firstRankedType.getRank())</span></a>
<a name="548"><span class="lineNum">     548 </span><span class="lineCov">          2 :         return emitOptionalError(location, &quot;dimension &quot;, dimension,</span></a>
<a name="549"><span class="lineNum">     549 </span>            :                                  &quot; is out-of-bounds for input rank &quot;,</a>
<a name="550"><span class="lineNum">     550 </span><span class="lineCov">          1 :                                  firstRankedType.getRank());</span></a>
<a name="551"><span class="lineNum">     551 </span><span class="lineCov">         75 :       continue;</span></a>
<a name="552"><span class="lineNum">     552 </span>            :     }</a>
<a name="553"><span class="lineNum">     553 </span><span class="lineCov">         76 :     if (firstRankedType.getRank() != secondType.getRank())</span></a>
<a name="554"><span class="lineNum">     554 </span><span class="lineCov">          1 :       return emitOptionalError(location, &quot;operands (&quot;, firstRankedIndex,</span></a>
<a name="555"><span class="lineNum">     555 </span>            :                                &quot;) and (&quot;, i, &quot;) do not match rank&quot;);</a>
<a name="556"><span class="lineNum">     556 </span>            : </a>
<a name="557"><span class="lineNum">     557 </span><span class="lineCov">         75 :     auto firstShape = firstRankedType.getShape();</span></a>
<a name="558"><span class="lineNum">     558 </span><span class="lineCov">         75 :     auto secondShape = secondType.getShape();</span></a>
<a name="559"><span class="lineNum">     559 </span><span class="lineCov">        219 :     for (int d = 0; d &lt; firstRankedType.getRank(); ++d) {</span></a>
<a name="560"><span class="lineNum">     560 </span><span class="lineCov">        232 :       if (!ShapedType::isDynamic(firstShape[d]) &amp;&amp;</span></a>
<a name="561"><span class="lineNum">     561 </span><span class="lineCov">        120 :           !ShapedType::isDynamic(secondShape[d]) &amp;&amp;</span></a>
<a name="562"><span class="lineNum">     562 </span><span class="lineCov">        112 :           firstShape[d] != secondShape[d] &amp;&amp; d != dimension) {</span></a>
<a name="563"><span class="lineNum">     563 </span><span class="lineCov">          1 :         return emitOptionalError(</span></a>
<a name="564"><span class="lineNum">     564 </span><span class="lineCov">          1 :             location, &quot;shapes of operand (&quot;, firstRankedIndex, &quot;) and (&quot;, i,</span></a>
<a name="565"><span class="lineNum">     565 </span>            :             &quot;) do not match at non-concat &quot;</a>
<a name="566"><span class="lineNum">     566 </span>            :             &quot;index: (&quot;,</a>
<a name="567"><span class="lineNum">     567 </span><span class="lineCov">          1 :             llvm::make_range(firstShape.begin(), firstShape.end()), &quot;) != (&quot;,</span></a>
<a name="568"><span class="lineNum">     568 </span><span class="lineCov">          1 :             llvm::make_range(secondShape.begin(), secondShape.end()),</span></a>
<a name="569"><span class="lineNum">     569 </span>            :             &quot;) at non-concat index &quot;, d);</a>
<a name="570"><span class="lineNum">     570 </span>            :       }</a>
<a name="571"><span class="lineNum">     571 </span><span class="lineCov">        143 :     }</span></a>
<a name="572"><span class="lineNum">     572 </span><span class="lineCov">        165 :   }</span></a>
<a name="573"><span class="lineNum">     573 </span>            : </a>
<a name="574"><span class="lineNum">     574 </span><span class="lineCov">         73 :   auto elementType = inputs[0].getType().cast&lt;ShapedType&gt;().getElementType();</span></a>
<a name="575"><span class="lineNum">     575 </span><span class="lineCov">         73 :   if (!firstRankedType) {</span></a>
<a name="576"><span class="lineNum">     576 </span><span class="lineNoCov">          0 :     inferredReturnTypes.push_back(UnrankedTensorType::get(elementType));</span></a>
<a name="577"><span class="lineNum">     577 </span><span class="lineNoCov">          0 :     return success();</span></a>
<a name="578"><span class="lineNum">     578 </span>            :   }</a>
<a name="579"><span class="lineNum">     579 </span>            : </a>
<a name="580"><span class="lineNum">     580 </span>            :   // Infer the most specific (size, bound) of all dimensions of the return type</a>
<a name="581"><span class="lineNum">     581 </span><span class="lineCov">         73 :   auto rank = firstRankedType.getRank();</span></a>
<a name="582"><span class="lineNum">     582 </span><span class="lineCov">         73 :   SmallVector&lt;int64_t&gt; inferredSizes(rank, ShapedType::kDynamicSize);</span></a>
<a name="583"><span class="lineNum">     583 </span><span class="lineCov">         73 :   SmallVector&lt;int64_t&gt; inferredBounds(rank, ShapedType::kDynamicSize);</span></a>
<a name="584"><span class="lineNum">     584 </span>            :   // Note: for the concatenate dimension, 0 should be the identity element:</a>
<a name="585"><span class="lineNum">     585 </span>            :   // Any dim size can keep unchanged when concatenated with 0</a>
<a name="586"><span class="lineNum">     586 </span><span class="lineCov">         73 :   inferredSizes[dimension] = 0;</span></a>
<a name="587"><span class="lineNum">     587 </span><span class="lineCov">         73 :   bool anyInputHaveBounds = false;</span></a>
<a name="588"><span class="lineNum">     588 </span>            : </a>
<a name="589"><span class="lineNum">     589 </span>            :   // Note: unranked input types can't be ignored, consider these input types:</a>
<a name="590"><span class="lineNum">     590 </span>            :   // c0: (&lt;5x?xf32&gt;, &lt;*xf32&gt;) with concat dim 0 should infer &lt;?x?xf32&gt;</a>
<a name="591"><span class="lineNum">     591 </span>            :   // c1: (&lt;5x?xf32&gt;, &lt;*xf32&gt;) with concat dim 1 should infer &lt;5x?xf32&gt;</a>
<a name="592"><span class="lineNum">     592 </span>            :   // Instead, they should be replaced with dynamic tensors: tensor&lt;?x...?x&gt;</a>
<a name="593"><span class="lineNum">     593 </span><span class="lineCov">        232 :   for (const auto&amp; it : llvm::enumerate(inputs.getTypes())) {</span></a>
<a name="594"><span class="lineNum">     594 </span><span class="lineCov">        159 :     RankedTensorType rankedType = it.value().dyn_cast&lt;RankedTensorType&gt;();</span></a>
<a name="595"><span class="lineNum">     595 </span><span class="lineCov">        159 :     SmallVector&lt;int64_t&gt; bounds;</span></a>
<a name="596"><span class="lineNum">     596 </span><span class="lineCov">        159 :     if (rankedType)</span></a>
<a name="597"><span class="lineNum">     597 </span><span class="lineCov">        147 :       bounds = to_vector(encodingToBounds(rankedType.getEncoding()));</span></a>
<a name="598"><span class="lineNum">     598 </span><span class="lineCov">        159 :     if (!bounds.empty()) anyInputHaveBounds = true;</span></a>
<a name="599"><span class="lineNum">     599 </span>            : </a>
<a name="600"><span class="lineNum">     600 </span><span class="lineCov">        457 :     for (int dim = 0; dim &lt; rank; ++dim) {</span></a>
<a name="601"><span class="lineNum">     601 </span><span class="lineCov">        298 :       std::pair&lt;int64_t, int64_t&gt; inferredDimAndBound;</span></a>
<a name="602"><span class="lineNum">     602 </span>            : </a>
<a name="603"><span class="lineNum">     603 </span><span class="lineCov">        298 :       int64_t leftSize = inferredSizes[dim];</span></a>
<a name="604"><span class="lineNum">     604 </span><span class="lineCov">        596 :       int64_t rightSize =</span></a>
<a name="605"><span class="lineNum">     605 </span><span class="lineCov">        298 :           rankedType ? rankedType.getShape()[dim] : ShapedType::kDynamicSize;</span></a>
<a name="606"><span class="lineNum">     606 </span><span class="lineCov">        298 :       int64_t leftBound = inferredBounds[dim];</span></a>
<a name="607"><span class="lineNum">     607 </span><span class="lineCov">        596 :       int64_t rightBound =</span></a>
<a name="608"><span class="lineNum">     608 </span><span class="lineCov">        298 :           bounds.empty() ? ShapedType::kDynamicSize : bounds[dim];</span></a>
<a name="609"><span class="lineNum">     609 </span><span class="lineCov">        298 :       if (dim == dimension) {</span></a>
<a name="610"><span class="lineNum">     610 </span><span class="lineCov">        318 :         inferredDimAndBound = inferConcatenatedDimAndBound(</span></a>
<a name="611"><span class="lineNum">     611 </span><span class="lineCov">        159 :             leftSize, rightSize, leftBound, rightBound);</span></a>
<a name="612"><span class="lineNum">     612 </span><span class="lineCov">        159 :       } else {</span></a>
<a name="613"><span class="lineNum">     613 </span><span class="lineCov">        278 :         auto inferredDimAndBoundOrErr = inferMergedDimAndBound(</span></a>
<a name="614"><span class="lineNum">     614 </span><span class="lineCov">        139 :             location, dim, leftSize, rightSize, leftBound, rightBound);</span></a>
<a name="615"><span class="lineNum">     615 </span><span class="lineCov">        139 :         if (failed(inferredDimAndBoundOrErr)) return failure();</span></a>
<a name="616"><span class="lineNum">     616 </span><span class="lineCov">        139 :         inferredDimAndBound = *inferredDimAndBoundOrErr;</span></a>
<a name="617"><span class="lineNum">     617 </span><span class="lineCov">        139 :       }</span></a>
<a name="618"><span class="lineNum">     618 </span><span class="lineCov">        298 :       inferredSizes[dim] = inferredDimAndBound.first;</span></a>
<a name="619"><span class="lineNum">     619 </span><span class="lineCov">        298 :       inferredBounds[dim] = inferredDimAndBound.second;</span></a>
<a name="620"><span class="lineNum">     620 </span><span class="lineCov">        298 :     }</span></a>
<a name="621"><span class="lineNum">     621 </span><span class="lineCov">        159 :   }</span></a>
<a name="622"><span class="lineNum">     622 </span>            : </a>
<a name="623"><span class="lineNum">     623 </span><span class="lineCov">        146 :   inferredReturnTypes.push_back(RankedTensorType::get(</span></a>
<a name="624"><span class="lineNum">     624 </span><span class="lineCov">         73 :       inferredSizes, elementType,</span></a>
<a name="625"><span class="lineNum">     625 </span><span class="lineCov">         73 :       boundsToEncoding(</span></a>
<a name="626"><span class="lineNum">     626 </span><span class="lineCov">         73 :           firstRankedType.getEncoding(),</span></a>
<a name="627"><span class="lineNum">     627 </span>            :           // Empty array as argument is an indicator to boundsToEncoding() that</a>
<a name="628"><span class="lineNum">     628 </span>            :           // there are no bounds at all in inputs, thus sparsity attributes will</a>
<a name="629"><span class="lineNum">     629 </span>            :           // be included in the return type</a>
<a name="630"><span class="lineNum">     630 </span><span class="lineCov">         73 :           anyInputHaveBounds ? inferredBounds : llvm::ArrayRef&lt;int64_t&gt;({}))));</span></a>
<a name="631"><span class="lineNum">     631 </span><span class="lineCov">         73 :   return success();</span></a>
<a name="632"><span class="lineNum">     632 </span><span class="lineCov">         79 : }</span></a>
<a name="633"><span class="lineNum">     633 </span>            : </a>
<a name="634"><span class="lineNum">     634 </span><span class="lineCov">         58 : LogicalResult inferDotGeneralOp(</span></a>
<a name="635"><span class="lineNum">     635 </span>            :     Optional&lt;Location&gt; location, Value lhs, Value rhs,</a>
<a name="636"><span class="lineNum">     636 </span>            :     ArrayRef&lt;int64_t&gt; lhsBatchingDimensions,</a>
<a name="637"><span class="lineNum">     637 </span>            :     ArrayRef&lt;int64_t&gt; rhsBatchingDimensions,</a>
<a name="638"><span class="lineNum">     638 </span>            :     ArrayRef&lt;int64_t&gt; lhsContractingDimensions,</a>
<a name="639"><span class="lineNum">     639 </span>            :     ArrayRef&lt;int64_t&gt; rhsContractingDimensions,</a>
<a name="640"><span class="lineNum">     640 </span><span class="lineCov">         58 :     SmallVectorImpl&lt;ShapedTypeComponents&gt;&amp; inferredReturnShapes) {</span></a>
<a name="641"><span class="lineNum">     641 </span><span class="lineCov">         58 :   if (lhsBatchingDimensions.size() != rhsBatchingDimensions.size())</span></a>
<a name="642"><span class="lineNum">     642 </span><span class="lineCov">          2 :     return emitOptionalError(location,</span></a>
<a name="643"><span class="lineNum">     643 </span>            :                              &quot;lhs and rhs should have the same &quot;</a>
<a name="644"><span class="lineNum">     644 </span>            :                              &quot;number of batching dimensions&quot;);</a>
<a name="645"><span class="lineNum">     645 </span><span class="lineCov">         56 :   if (lhsContractingDimensions.size() != rhsContractingDimensions.size())</span></a>
<a name="646"><span class="lineNum">     646 </span><span class="lineCov">          2 :     return emitOptionalError(location,</span></a>
<a name="647"><span class="lineNum">     647 </span>            :                              &quot;lhs and rhs should have the same &quot;</a>
<a name="648"><span class="lineNum">     648 </span>            :                              &quot;number of contracting dimensions&quot;);</a>
<a name="649"><span class="lineNum">     649 </span>            : </a>
<a name="650"><span class="lineNum">     650 </span><span class="lineCov">         54 :   llvm::SmallDenseSet&lt;int64_t&gt; dimSet;</span></a>
<a name="651"><span class="lineNum">     651 </span><span class="lineCov">         54 :   auto checkDimsDistinct =</span></a>
<a name="652"><span class="lineNum">     652 </span><span class="lineCov">        156 :       [&amp;](ArrayRef&lt;int64_t&gt; batchingDims, ArrayRef&lt;int64_t&gt; contractingDims,</span></a>
<a name="653"><span class="lineNum">     653 </span>            :           llvm::SmallDenseSet&lt;int64_t&gt;&amp; dimSet, llvm::StringRef lhs,</a>
<a name="654"><span class="lineNum">     654 </span><span class="lineCov">        102 :           llvm::StringRef rhs) -&gt; LogicalResult {</span></a>
<a name="655"><span class="lineNum">     655 </span><span class="lineCov">        102 :     auto dims = llvm::concat&lt;const int64_t&gt;(batchingDims, contractingDims);</span></a>
<a name="656"><span class="lineNum">     656 </span><span class="lineCov">        296 :     for (auto dim : dims) {</span></a>
<a name="657"><span class="lineNum">     657 </span><span class="lineCov">        194 :       auto [_, wasInserted] = dimSet.insert(dim);</span></a>
<a name="658"><span class="lineNum">     658 </span><span class="lineCov">        194 :       if (!wasInserted)</span></a>
<a name="659"><span class="lineNum">     659 </span><span class="lineCov">          8 :         return emitOptionalError(location, &quot;has duplicated dimension from &quot;,</span></a>
<a name="660"><span class="lineNum">     660 </span>            :                                  lhs, &quot; and &quot;, rhs, &quot;: &quot;, dim);</a>
<a name="661"><span class="lineNum">     661 </span><span class="lineCov">        194 :     }</span></a>
<a name="662"><span class="lineNum">     662 </span><span class="lineCov">         94 :     return success();</span></a>
<a name="663"><span class="lineNum">     663 </span><span class="lineCov">        102 :   };</span></a>
<a name="664"><span class="lineNum">     664 </span>            : </a>
<a name="665"><span class="lineNum">     665 </span><span class="lineCov">        108 :   if (failed(checkDimsDistinct(lhsBatchingDimensions, lhsContractingDimensions,</span></a>
<a name="666"><span class="lineNum">     666 </span><span class="lineCov">         54 :                                dimSet, &quot;lhs_batching_dimensions&quot;,</span></a>
<a name="667"><span class="lineNum">     667 </span><span class="lineCov">         54 :                                &quot;lhs_contracting_dimensions&quot;)))</span></a>
<a name="668"><span class="lineNum">     668 </span><span class="lineCov">          6 :     return failure();</span></a>
<a name="669"><span class="lineNum">     669 </span>            : </a>
<a name="670"><span class="lineNum">     670 </span><span class="lineCov">         48 :   dimSet.clear();</span></a>
<a name="671"><span class="lineNum">     671 </span>            : </a>
<a name="672"><span class="lineNum">     672 </span><span class="lineCov">         96 :   if (failed(checkDimsDistinct(rhsBatchingDimensions, rhsContractingDimensions,</span></a>
<a name="673"><span class="lineNum">     673 </span><span class="lineCov">         48 :                                dimSet, &quot;rhs_batching_dimensions&quot;,</span></a>
<a name="674"><span class="lineNum">     674 </span><span class="lineCov">         48 :                                &quot;rhs_contracting_dimensions&quot;)))</span></a>
<a name="675"><span class="lineNum">     675 </span><span class="lineCov">          2 :     return failure();</span></a>
<a name="676"><span class="lineNum">     676 </span>            : </a>
<a name="677"><span class="lineNum">     677 </span><span class="lineCov">        202 :   auto checkDimsInRange = [&amp;](int64_t rank, ArrayRef&lt;int64_t&gt; dims,</span></a>
<a name="678"><span class="lineNum">     678 </span><span class="lineCov">        156 :                               llvm::StringRef dimName) -&gt; LogicalResult {</span></a>
<a name="679"><span class="lineNum">     679 </span><span class="lineCov">        300 :     auto inRange = [&amp;](int64_t i) -&gt; bool { return 0 &lt;= i &amp;&amp; i &lt; rank; };</span></a>
<a name="680"><span class="lineNum">     680 </span><span class="lineCov">        312 :     const auto* dimsNotInRange =</span></a>
<a name="681"><span class="lineNum">     681 </span><span class="lineCov">        156 :         std::find_if_not(dims.begin(), dims.end(), inRange);</span></a>
<a name="682"><span class="lineNum">     682 </span><span class="lineCov">        156 :     if (dimsNotInRange != dims.end())</span></a>
<a name="683"><span class="lineNum">     683 </span><span class="lineCov">          8 :       return emitOptionalError(location, dimName, &quot; value: &quot;, *dimsNotInRange,</span></a>
<a name="684"><span class="lineNum">     684 </span>            :                                &quot; is out of range: &quot;, &quot;[0, &quot;, rank, &quot;)&quot;);</a>
<a name="685"><span class="lineNum">     685 </span><span class="lineCov">        148 :     return success();</span></a>
<a name="686"><span class="lineNum">     686 </span><span class="lineCov">        156 :   };</span></a>
<a name="687"><span class="lineNum">     687 </span><span class="lineCov">         46 :   auto lhsRankedType = lhs.getType().dyn_cast&lt;RankedTensorType&gt;();</span></a>
<a name="688"><span class="lineNum">     688 </span><span class="lineCov">         46 :   auto rhsRankedType = rhs.getType().dyn_cast&lt;RankedTensorType&gt;();</span></a>
<a name="689"><span class="lineNum">     689 </span>            : </a>
<a name="690"><span class="lineNum">     690 </span><span class="lineCov">         46 :   if (lhsRankedType) {</span></a>
<a name="691"><span class="lineNum">     691 </span><span class="lineCov">         88 :     if (failed(checkDimsInRange(lhsRankedType.getRank(), lhsBatchingDimensions,</span></a>
<a name="692"><span class="lineNum">     692 </span><span class="lineCov">         88 :                                 &quot;lhs_batching_dimensions&quot;)) ||</span></a>
<a name="693"><span class="lineNum">     693 </span><span class="lineCov">         84 :         failed(checkDimsInRange(lhsRankedType.getRank(),</span></a>
<a name="694"><span class="lineNum">     694 </span><span class="lineCov">         42 :                                 lhsContractingDimensions,</span></a>
<a name="695"><span class="lineNum">     695 </span><span class="lineCov">         42 :                                 &quot;lhs_contracting_dimensions&quot;)))</span></a>
<a name="696"><span class="lineNum">     696 </span><span class="lineCov">          4 :       return failure();</span></a>
<a name="697"><span class="lineNum">     697 </span><span class="lineCov">         40 :   }</span></a>
<a name="698"><span class="lineNum">     698 </span><span class="lineCov">         42 :   if (rhsRankedType) {</span></a>
<a name="699"><span class="lineNum">     699 </span><span class="lineCov">         72 :     if (failed(checkDimsInRange(rhsRankedType.getRank(), rhsBatchingDimensions,</span></a>
<a name="700"><span class="lineNum">     700 </span><span class="lineCov">         72 :                                 &quot;rhs_batching_dimensions&quot;)) ||</span></a>
<a name="701"><span class="lineNum">     701 </span><span class="lineCov">         68 :         failed(checkDimsInRange(rhsRankedType.getRank(),</span></a>
<a name="702"><span class="lineNum">     702 </span><span class="lineCov">         34 :                                 rhsContractingDimensions,</span></a>
<a name="703"><span class="lineNum">     703 </span><span class="lineCov">         34 :                                 &quot;rhs_contracting_dimensions&quot;)))</span></a>
<a name="704"><span class="lineNum">     704 </span><span class="lineCov">          4 :       return failure();</span></a>
<a name="705"><span class="lineNum">     705 </span><span class="lineCov">         32 :   }</span></a>
<a name="706"><span class="lineNum">     706 </span><span class="lineCov">         38 :   if (lhsRankedType &amp;&amp; rhsRankedType) {</span></a>
<a name="707"><span class="lineNum">     707 </span>            :     // Dimension sizes must be compatible for lhs/rhs.</a>
<a name="708"><span class="lineNum">     708 </span><span class="lineCov">         32 :     auto lhsShape = lhsRankedType.getShape();</span></a>
<a name="709"><span class="lineNum">     709 </span><span class="lineCov">         32 :     auto rhsShape = rhsRankedType.getShape();</span></a>
<a name="710"><span class="lineNum">     710 </span>            : </a>
<a name="711"><span class="lineNum">     711 </span><span class="lineCov">         58 :     for (auto [lhs, rhs] :</span></a>
<a name="712"><span class="lineNum">     712 </span><span class="lineCov">         32 :          llvm::zip(lhsBatchingDimensions, rhsBatchingDimensions))</span></a>
<a name="713"><span class="lineNum">     713 </span><span class="lineCov">         52 :       if (lhsShape[lhs] != rhsShape[rhs])</span></a>
<a name="714"><span class="lineNum">     714 </span><span class="lineCov">          1 :         return emitOptionalError(location,</span></a>
<a name="715"><span class="lineNum">     715 </span>            :                                  &quot;batching dimension sizes must &quot;</a>
<a name="716"><span class="lineNum">     716 </span><span class="lineCov">         26 :                                  &quot;match for lhs/rhs&quot;);</span></a>
<a name="717"><span class="lineNum">     717 </span><span class="lineCov">         62 :     for (auto [lhs, rhs] :</span></a>
<a name="718"><span class="lineNum">     718 </span><span class="lineCov">         31 :          llvm::zip(lhsContractingDimensions, rhsContractingDimensions))</span></a>
<a name="719"><span class="lineNum">     719 </span><span class="lineCov">         62 :       if (lhsShape[lhs] != rhsShape[rhs])</span></a>
<a name="720"><span class="lineNum">     720 </span><span class="lineCov">          1 :         return emitOptionalError(location,</span></a>
<a name="721"><span class="lineNum">     721 </span>            :                                  &quot;contracting dimension sizes must &quot;</a>
<a name="722"><span class="lineNum">     722 </span><span class="lineCov">         31 :                                  &quot;match for lhs/rhs&quot;);</span></a>
<a name="723"><span class="lineNum">     723 </span><span class="lineCov">         32 :   }</span></a>
<a name="724"><span class="lineNum">     724 </span>            : </a>
<a name="725"><span class="lineNum">     725 </span><span class="lineCov">         36 :   auto lhsType = lhs.getType().cast&lt;ShapedType&gt;();</span></a>
<a name="726"><span class="lineNum">     726 </span><span class="lineCov">         36 :   auto rhsType = rhs.getType().cast&lt;ShapedType&gt;();</span></a>
<a name="727"><span class="lineNum">     727 </span><span class="lineCov">         36 :   auto elementType = lhsType.getElementType();</span></a>
<a name="728"><span class="lineNum">     728 </span>            : </a>
<a name="729"><span class="lineNum">     729 </span><span class="lineCov">         36 :   if (!lhsType.hasRank() || !rhsType.hasRank()) {</span></a>
<a name="730"><span class="lineNum">     730 </span><span class="lineCov">          6 :     inferredReturnShapes.emplace_back(elementType);</span></a>
<a name="731"><span class="lineNum">     731 </span><span class="lineCov">          6 :     return success();</span></a>
<a name="732"><span class="lineNum">     732 </span>            :   }</a>
<a name="733"><span class="lineNum">     733 </span>            : </a>
<a name="734"><span class="lineNum">     734 </span><span class="lineCov">         30 :   auto lhsShape = lhsType.getShape();</span></a>
<a name="735"><span class="lineNum">     735 </span><span class="lineCov">         30 :   auto rhsShape = rhsType.getShape();</span></a>
<a name="736"><span class="lineNum">     736 </span>            : </a>
<a name="737"><span class="lineNum">     737 </span>            :   // Infer the output dimensions of the operation.</a>
<a name="738"><span class="lineNum">     738 </span><span class="lineCov">         30 :   SmallVector&lt;int64_t&gt; dimensions;</span></a>
<a name="739"><span class="lineNum">     739 </span><span class="lineCov">         54 :   for (const int64_t lhsBatchingDim : lhsBatchingDimensions)</span></a>
<a name="740"><span class="lineNum">     740 </span><span class="lineCov">         24 :     dimensions.push_back(lhsShape[lhsBatchingDim]);</span></a>
<a name="741"><span class="lineNum">     741 </span><span class="lineCov">        108 :   for (int64_t i = 0; i &lt; lhsType.getRank(); i++)</span></a>
<a name="742"><span class="lineNum">     742 </span><span class="lineCov">         78 :     if (!llvm::is_contained(lhsBatchingDimensions, i) &amp;&amp;</span></a>
<a name="743"><span class="lineNum">     743 </span><span class="lineCov">        102 :         !llvm::is_contained(lhsContractingDimensions, i))</span></a>
<a name="744"><span class="lineNum">     744 </span><span class="lineCov">         24 :       dimensions.push_back(lhsShape[i]);</span></a>
<a name="745"><span class="lineNum">     745 </span><span class="lineCov">        108 :   for (int64_t i = 0; i &lt; rhsType.getRank(); i++)</span></a>
<a name="746"><span class="lineNum">     746 </span><span class="lineCov">         78 :     if (!llvm::is_contained(rhsBatchingDimensions, i) &amp;&amp;</span></a>
<a name="747"><span class="lineNum">     747 </span><span class="lineCov">        102 :         !llvm::is_contained(rhsContractingDimensions, i))</span></a>
<a name="748"><span class="lineNum">     748 </span><span class="lineCov">         24 :       dimensions.push_back(rhsShape[i]);</span></a>
<a name="749"><span class="lineNum">     749 </span>            : </a>
<a name="750"><span class="lineNum">     750 </span><span class="lineCov">         30 :   inferredReturnShapes.emplace_back(dimensions, elementType);</span></a>
<a name="751"><span class="lineNum">     751 </span><span class="lineCov">         30 :   return success();</span></a>
<a name="752"><span class="lineNum">     752 </span><span class="lineCov">         58 : }</span></a>
<a name="753"><span class="lineNum">     753 </span>            : </a>
<a name="754"><span class="lineNum">     754 </span><span class="lineCov">         20 : LogicalResult inferIfOp(Optional&lt;Location&gt; location, RegionRange branches,</span></a>
<a name="755"><span class="lineNum">     755 </span><span class="lineCov">         20 :                         SmallVectorImpl&lt;Type&gt;&amp; inferredReturnTypes) {</span></a>
<a name="756"><span class="lineNum">     756 </span><span class="lineCov">         20 :   return inferConditionalOp(location, branches, inferredReturnTypes);</span></a>
<a name="757"><span class="lineNum">     757 </span>            : }</a>
<a name="758"><span class="lineNum">     758 </span>            : </a>
<a name="759"><span class="lineNum">     759 </span><span class="lineCov">         46 : LogicalResult inferMapOp(</span></a>
<a name="760"><span class="lineNum">     760 </span>            :     Optional&lt;Location&gt; location, ValueRange inputs,</a>
<a name="761"><span class="lineNum">     761 </span>            :     DenseIntElementsAttr dimensions, Region&amp; computation,</a>
<a name="762"><span class="lineNum">     762 </span><span class="lineCov">         46 :     SmallVectorImpl&lt;ShapedTypeComponents&gt;&amp; inferredReturnShapes) {</span></a>
<a name="763"><span class="lineNum">     763 </span>            :   // Checks if the number of `operands` match the arity of the map `computation`</a>
<a name="764"><span class="lineNum">     764 </span>            :   // region.</a>
<a name="765"><span class="lineNum">     765 </span><span class="lineCov">         46 :   auto&amp; computationBlock = computation.front();</span></a>
<a name="766"><span class="lineNum">     766 </span><span class="lineCov">         46 :   auto computationArgs = computationBlock.getArguments();</span></a>
<a name="767"><span class="lineNum">     767 </span><span class="lineCov">         46 :   if (inputs.size() != computationArgs.size())</span></a>
<a name="768"><span class="lineNum">     768 </span><span class="lineCov">          2 :     return emitOptionalError(location,</span></a>
<a name="769"><span class="lineNum">     769 </span>            :                              &quot;expects number of operands to match the arity of &quot;</a>
<a name="770"><span class="lineNum">     770 </span>            :                              &quot;map computation, but got: &quot;,</a>
<a name="771"><span class="lineNum">     771 </span><span class="lineCov">          1 :                              inputs.size(), &quot; and &quot;, computationArgs.size());</span></a>
<a name="772"><span class="lineNum">     772 </span>            : </a>
<a name="773"><span class="lineNum">     773 </span>            :   // The parameters of computation should all be scalars and match the element</a>
<a name="774"><span class="lineNum">     774 </span>            :   // type of operands.</a>
<a name="775"><span class="lineNum">     775 </span><span class="lineCov">        134 :   for (const auto&amp; indexedArg : llvm::enumerate(computationArgs)) {</span></a>
<a name="776"><span class="lineNum">     776 </span><span class="lineCov">         89 :     auto argType = indexedArg.value().getType().dyn_cast&lt;RankedTensorType&gt;();</span></a>
<a name="777"><span class="lineNum">     777 </span><span class="lineCov">         89 :     if (!argType || argType.getRank() != 0)</span></a>
<a name="778"><span class="lineNum">     778 </span><span class="lineCov">          1 :       return emitOptionalError(</span></a>
<a name="779"><span class="lineNum">     779 </span><span class="lineCov">          1 :           location,</span></a>
<a name="780"><span class="lineNum">     780 </span>            :           &quot;computation arguments must be 0-rank tensor, but got: arg #&quot;,</a>
<a name="781"><span class="lineNum">     781 </span><span class="lineCov">          1 :           indexedArg.index(), &quot; of type &quot;, indexedArg.value().getType());</span></a>
<a name="782"><span class="lineNum">     782 </span><span class="lineCov">        176 :     auto operandElemTy = inputs[indexedArg.index()]</span></a>
<a name="783"><span class="lineNum">     783 </span><span class="lineCov">         88 :                              .getType()</span></a>
<a name="784"><span class="lineNum">     784 </span><span class="lineCov">         88 :                              .cast&lt;TensorType&gt;()</span></a>
<a name="785"><span class="lineNum">     785 </span><span class="lineCov">         88 :                              .getElementType();</span></a>
<a name="786"><span class="lineNum">     786 </span><span class="lineCov">         88 :     if (argType.getElementType() != operandElemTy) {</span></a>
<a name="787"><span class="lineNum">     787 </span><span class="lineCov">          2 :       return emitOptionalError(location,</span></a>
<a name="788"><span class="lineNum">     788 </span>            :                                &quot;element type of operands and computation &quot;</a>
<a name="789"><span class="lineNum">     789 </span>            :                                &quot;arguments must match, but got: &quot;,</a>
<a name="790"><span class="lineNum">     790 </span>            :                                operandElemTy, &quot; and &quot;,</a>
<a name="791"><span class="lineNum">     791 </span><span class="lineCov">          1 :                                argType.getElementType());</span></a>
<a name="792"><span class="lineNum">     792 </span>            :     }</a>
<a name="793"><span class="lineNum">     793 </span><span class="lineCov">         89 :   }</span></a>
<a name="794"><span class="lineNum">     794 </span>            : </a>
<a name="795"><span class="lineNum">     795 </span>            :   // Mapped computation must return single output</a>
<a name="796"><span class="lineNum">     796 </span><span class="lineCov">         43 :   auto computationOutputs = computationBlock.getTerminator()-&gt;getOperands();</span></a>
<a name="797"><span class="lineNum">     797 </span><span class="lineCov">         43 :   if (computationOutputs.size() != 1)</span></a>
<a name="798"><span class="lineNum">     798 </span><span class="lineCov">          2 :     return emitOptionalError(location,</span></a>
<a name="799"><span class="lineNum">     799 </span>            :                              &quot;computation must return single output, but got: &quot;,</a>
<a name="800"><span class="lineNum">     800 </span><span class="lineCov">          1 :                              computationOutputs.size());</span></a>
<a name="801"><span class="lineNum">     801 </span>            : </a>
<a name="802"><span class="lineNum">     802 </span>            :   // The output of computation must be scalar and have the same element type</a>
<a name="803"><span class="lineNum">     803 </span>            :   // as op result.</a>
<a name="804"><span class="lineNum">     804 </span><span class="lineCov">         42 :   auto computationOutputType =</span></a>
<a name="805"><span class="lineNum">     805 </span><span class="lineCov">         42 :       computationOutputs[0].getType().dyn_cast&lt;RankedTensorType&gt;();</span></a>
<a name="806"><span class="lineNum">     806 </span><span class="lineCov">         42 :   if (!computationOutputType || computationOutputType.getRank() != 0)</span></a>
<a name="807"><span class="lineNum">     807 </span><span class="lineCov">          2 :     return emitOptionalError(location,</span></a>
<a name="808"><span class="lineNum">     808 </span>            :                              &quot;computation must return 0-rank tensor, but got: &quot;,</a>
<a name="809"><span class="lineNum">     809 </span><span class="lineCov">          1 :                              computationOutputs[0].getType());</span></a>
<a name="810"><span class="lineNum">     810 </span>            : </a>
<a name="811"><span class="lineNum">     811 </span>            :   // Checks that the requested map dimension numbers are monotonically</a>
<a name="812"><span class="lineNum">     812 </span>            :   // increasing.</a>
<a name="813"><span class="lineNum">     813 </span><span class="lineCov">         89 :   for (const auto&amp; indexedValue :</span></a>
<a name="814"><span class="lineNum">     814 </span><span class="lineCov">         41 :        llvm::enumerate(dimensions.getValues&lt;int64_t&gt;())) {</span></a>
<a name="815"><span class="lineNum">     815 </span><span class="lineCov">         48 :     if (indexedValue.value() != static_cast&lt;int64_t&gt;(indexedValue.index()))</span></a>
<a name="816"><span class="lineNum">     816 </span><span class="lineCov">          1 :       return emitOptionalError(</span></a>
<a name="817"><span class="lineNum">     817 </span><span class="lineCov">          1 :           location,</span></a>
<a name="818"><span class="lineNum">     818 </span>            :           &quot;requires monotonically increasing dimension numbers, but got: &quot;,</a>
<a name="819"><span class="lineNum">     819 </span>            :           dimensions);</a>
<a name="820"><span class="lineNum">     820 </span><span class="lineCov">         48 :   }</span></a>
<a name="821"><span class="lineNum">     821 </span>            : </a>
<a name="822"><span class="lineNum">     822 </span>            :   // Checks that number of dimensions of operands matches the size of</a>
<a name="823"><span class="lineNum">     823 </span>            :   // `dimensions` since we currently only support mapping across all</a>
<a name="824"><span class="lineNum">     824 </span>            :   // dimensions: i.e., scalar map functions.</a>
<a name="825"><span class="lineNum">     825 </span><span class="lineCov">         40 :   ArrayRef&lt;int64_t&gt; resultShape;</span></a>
<a name="826"><span class="lineNum">     826 </span><span class="lineCov">         40 :   bool allInputsUnranked = true;</span></a>
<a name="827"><span class="lineNum">     827 </span><span class="lineCov">        119 :   for (auto operand : inputs) {</span></a>
<a name="828"><span class="lineNum">     828 </span><span class="lineCov">         79 :     auto operandType = operand.getType().cast&lt;TensorType&gt;();</span></a>
<a name="829"><span class="lineNum">     829 </span><span class="lineCov">         79 :     if (operandType.hasRank()) {</span></a>
<a name="830"><span class="lineNum">     830 </span><span class="lineCov">        150 :       if (dimensions.size() !=</span></a>
<a name="831"><span class="lineNum">     831 </span><span class="lineCov">         75 :           static_cast&lt;int64_t&gt;(operandType.getShape().size()))</span></a>
<a name="832"><span class="lineNum">     832 </span><span class="lineCov">          1 :         return emitOptionalError(</span></a>
<a name="833"><span class="lineNum">     833 </span><span class="lineCov">          1 :             location,</span></a>
<a name="834"><span class="lineNum">     834 </span>            :             &quot;applied to a subset of dimensions currently not supported: &quot;</a>
<a name="835"><span class="lineNum">     835 </span>            :             &quot;operand dimensions = &quot;,</a>
<a name="836"><span class="lineNum">     836 </span><span class="lineCov">          1 :             operandType.getShape().size(),</span></a>
<a name="837"><span class="lineNum">     837 </span><span class="lineCov">          1 :             &quot;, requested map dimensions size = &quot;, dimensions.size());</span></a>
<a name="838"><span class="lineNum">     838 </span><span class="lineCov">         74 :       resultShape = operandType.getShape();</span></a>
<a name="839"><span class="lineNum">     839 </span><span class="lineCov">         74 :       allInputsUnranked = false;</span></a>
<a name="840"><span class="lineNum">     840 </span><span class="lineCov">         74 :     }</span></a>
<a name="841"><span class="lineNum">     841 </span><span class="lineCov">         79 :   }</span></a>
<a name="842"><span class="lineNum">     842 </span>            : </a>
<a name="843"><span class="lineNum">     843 </span><span class="lineCov">         39 :   if (allInputsUnranked)</span></a>
<a name="844"><span class="lineNum">     844 </span><span class="lineCov">          2 :     inferredReturnShapes.emplace_back(computationOutputType.getElementType());</span></a>
<a name="845"><span class="lineNum">     845 </span>            :   else</a>
<a name="846"><span class="lineNum">     846 </span><span class="lineCov">         74 :     inferredReturnShapes.emplace_back(resultShape,</span></a>
<a name="847"><span class="lineNum">     847 </span><span class="lineCov">         37 :                                       computationOutputType.getElementType());</span></a>
<a name="848"><span class="lineNum">     848 </span><span class="lineCov">         39 :   return success();</span></a>
<a name="849"><span class="lineNum">     849 </span><span class="lineCov">         46 : }</span></a>
<a name="850"><span class="lineNum">     850 </span>            : </a>
<a name="851"><span class="lineNum">     851 </span><span class="lineCov">         39 : LogicalResult inferPadOp(Optional&lt;Location&gt; location, Value operand,</span></a>
<a name="852"><span class="lineNum">     852 </span>            :                          Value paddingValue,</a>
<a name="853"><span class="lineNum">     853 </span>            :                          DenseIntElementsAttr edgePaddingLow,</a>
<a name="854"><span class="lineNum">     854 </span>            :                          DenseIntElementsAttr edgePaddingHigh,</a>
<a name="855"><span class="lineNum">     855 </span>            :                          DenseIntElementsAttr interiorPadding,</a>
<a name="856"><span class="lineNum">     856 </span><span class="lineCov">         39 :                          SmallVectorImpl&lt;Type&gt;&amp; inferredReturnTypes) {</span></a>
<a name="857"><span class="lineNum">     857 </span><span class="lineCov">         39 :   auto inputType = operand.getType().cast&lt;RankedTensorType&gt;();</span></a>
<a name="858"><span class="lineNum">     858 </span><span class="lineCov">         39 :   auto padType = paddingValue.getType().cast&lt;RankedTensorType&gt;();</span></a>
<a name="859"><span class="lineNum">     859 </span>            : </a>
<a name="860"><span class="lineNum">     860 </span><span class="lineCov">         39 :   if (padType.getRank() != 0)</span></a>
<a name="861"><span class="lineNum">     861 </span><span class="lineCov">          2 :     return emitOptionalError(location,</span></a>
<a name="862"><span class="lineNum">     862 </span>            :                              &quot;padding value type should be a rank-0 &quot;</a>
<a name="863"><span class="lineNum">     863 </span>            :                              &quot;tensor, is rank &quot;,</a>
<a name="864"><span class="lineNum">     864 </span><span class="lineCov">          1 :                              padType.getRank());</span></a>
<a name="865"><span class="lineNum">     865 </span>            : </a>
<a name="866"><span class="lineNum">     866 </span><span class="lineCov">         38 :   int64_t rank = inputType.getRank();</span></a>
<a name="867"><span class="lineNum">     867 </span><span class="lineCov">         38 :   if (edgePaddingLow.getType().getNumElements() != rank)</span></a>
<a name="868"><span class="lineNum">     868 </span><span class="lineCov">          2 :     return emitOptionalError(location, &quot;edge_padding_low length (&quot;,</span></a>
<a name="869"><span class="lineNum">     869 </span><span class="lineCov">          1 :                              edgePaddingLow.getType().getNumElements(),</span></a>
<a name="870"><span class="lineNum">     870 </span>            :                              &quot;) must match operand rank (&quot;, rank, &quot;)&quot;);</a>
<a name="871"><span class="lineNum">     871 </span>            : </a>
<a name="872"><span class="lineNum">     872 </span><span class="lineCov">         37 :   if (edgePaddingHigh.getType().getNumElements() != rank)</span></a>
<a name="873"><span class="lineNum">     873 </span><span class="lineCov">          2 :     return emitOptionalError(location, &quot;edge_padding_high length (&quot;,</span></a>
<a name="874"><span class="lineNum">     874 </span><span class="lineCov">          1 :                              edgePaddingHigh.getType().getNumElements(),</span></a>
<a name="875"><span class="lineNum">     875 </span>            :                              &quot;) must match operand rank (&quot;, rank, &quot;)&quot;);</a>
<a name="876"><span class="lineNum">     876 </span>            : </a>
<a name="877"><span class="lineNum">     877 </span><span class="lineCov">         36 :   if (interiorPadding.getType().getNumElements() != rank)</span></a>
<a name="878"><span class="lineNum">     878 </span><span class="lineCov">          2 :     return emitOptionalError(location, &quot;interior_padding length (&quot;,</span></a>
<a name="879"><span class="lineNum">     879 </span><span class="lineCov">          1 :                              interiorPadding.getType().getNumElements(),</span></a>
<a name="880"><span class="lineNum">     880 </span>            :                              &quot;) must match operand rank (&quot;, rank, &quot;)&quot;);</a>
<a name="881"><span class="lineNum">     881 </span>            : </a>
<a name="882"><span class="lineNum">     882 </span><span class="lineCov">         35 :   auto inputShape = inputType.getShape();</span></a>
<a name="883"><span class="lineNum">     883 </span><span class="lineCov">         35 :   SmallVector&lt;int64_t&gt; resultShape(rank, ShapedType::kDynamicSize);</span></a>
<a name="884"><span class="lineNum">     884 </span><span class="lineCov">         35 :   ArrayRef&lt;int64_t&gt; inputBounds = encodingToBounds(inputType.getEncoding());</span></a>
<a name="885"><span class="lineNum">     885 </span><span class="lineCov">         35 :   SmallVector&lt;int64_t&gt; resultBounds(inputBounds.size(),</span></a>
<a name="886"><span class="lineNum">     886 </span>            :                                     ShapedType::kDynamicSize);</a>
<a name="887"><span class="lineNum">     887 </span>            : </a>
<a name="888"><span class="lineNum">     888 </span><span class="lineCov">        116 :   for (int i = 0, e = inputShape.size(); i &lt; e; i++) {</span></a>
<a name="889"><span class="lineNum">     889 </span><span class="lineCov">         81 :     int64_t paddingLowVal = edgePaddingLow.getValues&lt;APInt&gt;()[i].getSExtValue();</span></a>
<a name="890"><span class="lineNum">     890 </span><span class="lineCov">        162 :     int64_t paddingHighVal =</span></a>
<a name="891"><span class="lineNum">     891 </span><span class="lineCov">         81 :         edgePaddingHigh.getValues&lt;APInt&gt;()[i].getSExtValue();</span></a>
<a name="892"><span class="lineNum">     892 </span><span class="lineCov">        162 :     int64_t paddingInteriorVal =</span></a>
<a name="893"><span class="lineNum">     893 </span><span class="lineCov">         81 :         interiorPadding.getValues&lt;APInt&gt;()[i].getSExtValue();</span></a>
<a name="894"><span class="lineNum">     894 </span><span class="lineCov">         81 :     if (paddingInteriorVal &lt; 0)</span></a>
<a name="895"><span class="lineNum">     895 </span><span class="lineCov">          1 :       return emitOptionalError(</span></a>
<a name="896"><span class="lineNum">     896 </span><span class="lineCov">          1 :           location,</span></a>
<a name="897"><span class="lineNum">     897 </span>            :           &quot;Interior padding cannot be negative: &quot;, paddingInteriorVal);</a>
<a name="898"><span class="lineNum">     898 </span>            : </a>
<a name="899"><span class="lineNum">     899 </span><span class="lineCov">         80 :     bool isStaticDim = !hlo::isDynamicDimSize(inputShape[i]);</span></a>
<a name="900"><span class="lineNum">     900 </span><span class="lineCov">         94 :     bool isStaticBound =</span></a>
<a name="901"><span class="lineNum">     901 </span><span class="lineCov">         80 :         !inputBounds.empty() &amp;&amp; !hlo::isDynamicDimSize(inputBounds[i]);</span></a>
<a name="902"><span class="lineNum">     902 </span><span class="lineCov">         80 :     if (isStaticDim || isStaticBound) {</span></a>
<a name="903"><span class="lineNum">     903 </span><span class="lineCov">         74 :       int64_t operandSizeOrBound = isStaticDim ? inputShape[i] : inputBounds[i];</span></a>
<a name="904"><span class="lineNum">     904 </span><span class="lineCov">        148 :       int64_t resultSizeOrBound =</span></a>
<a name="905"><span class="lineNum">     905 </span><span class="lineCov">        148 :           operandSizeOrBound + paddingLowVal + paddingHighVal +</span></a>
<a name="906"><span class="lineNum">     906 </span><span class="lineCov">         74 :           std::max&lt;int64_t&gt;(operandSizeOrBound - 1, 0LL) * paddingInteriorVal;</span></a>
<a name="907"><span class="lineNum">     907 </span>            : </a>
<a name="908"><span class="lineNum">     908 </span><span class="lineCov">         74 :       if (resultSizeOrBound &lt; 0) {</span></a>
<a name="909"><span class="lineNum">     909 </span><span class="lineCov">          2 :         auto sizeOrBound = isStaticDim ? &quot;size&quot; : &quot;bound&quot;;</span></a>
<a name="910"><span class="lineNum">     910 </span><span class="lineCov">          2 :         return emitOptionalError(location, &quot;Padding result in negative &quot;,</span></a>
<a name="911"><span class="lineNum">     911 </span>            :                                  sizeOrBound, &quot; for dimension &quot;, i);</a>
<a name="912"><span class="lineNum">     912 </span><span class="lineCov">          2 :       }</span></a>
<a name="913"><span class="lineNum">     913 </span><span class="lineCov">         72 :       (isStaticDim ? resultShape : resultBounds)[i] = resultSizeOrBound;</span></a>
<a name="914"><span class="lineNum">     914 </span><span class="lineCov">         74 :     }</span></a>
<a name="915"><span class="lineNum">     915 </span><span class="lineCov">         81 :   }</span></a>
<a name="916"><span class="lineNum">     916 </span><span class="lineCov">         64 :   inferredReturnTypes.push_back(RankedTensorType::get(</span></a>
<a name="917"><span class="lineNum">     917 </span><span class="lineCov">         32 :       resultShape, inputType.getElementType(),</span></a>
<a name="918"><span class="lineNum">     918 </span><span class="lineCov">         32 :       boundsToEncoding(inputType.getEncoding(), resultBounds)));</span></a>
<a name="919"><span class="lineNum">     919 </span>            : </a>
<a name="920"><span class="lineNum">     920 </span><span class="lineCov">         32 :   return success();</span></a>
<a name="921"><span class="lineNum">     921 </span><span class="lineCov">         39 : }</span></a>
<a name="922"><span class="lineNum">     922 </span>            : </a>
<a name="923"><span class="lineNum">     923 </span>            : // We intend to verify the following properties</a>
<a name="924"><span class="lineNum">     924 </span>            : //  P1. Verify all `inputs` need to have compatible shapes.</a>
<a name="925"><span class="lineNum">     925 </span>            : //  P2. Verify that</a>
<a name="926"><span class="lineNum">     926 </span>            : //      1. the dimensions of reduce-op are in-bounds for the given shape.</a>
<a name="927"><span class="lineNum">     927 </span>            : //      2. the dimension-attribute have no duplicate entries.</a>
<a name="928"><span class="lineNum">     928 </span>            : //  P3. Verify the inner block defining the reducer function.</a>
<a name="929"><span class="lineNum">     929 </span><span class="lineCov">        142 : LogicalResult inferReduceOp(</span></a>
<a name="930"><span class="lineNum">     930 </span>            :     Optional&lt;Location&gt; location, ValueRange inputs, ValueRange initValues,</a>
<a name="931"><span class="lineNum">     931 </span>            :     DenseIntElementsAttr dimensions, Region&amp; body,</a>
<a name="932"><span class="lineNum">     932 </span><span class="lineCov">        142 :     SmallVectorImpl&lt;ShapedTypeComponents&gt;&amp; inferredReturnShapes) {</span></a>
<a name="933"><span class="lineNum">     933 </span><span class="lineCov">        284 :   SmallVector&lt;TensorType&gt; inputArgTypes{llvm::map_range(</span></a>
<a name="934"><span class="lineNum">     934 </span><span class="lineCov">        142 :       inputs.getTypes(),</span></a>
<a name="935"><span class="lineNum">     935 </span><span class="lineCov">        191 :       [](Type t) -&gt; TensorType { return t.cast&lt;TensorType&gt;(); })};</span></a>
<a name="936"><span class="lineNum">     936 </span><span class="lineCov">        284 :   SmallVector&lt;TensorType&gt; initValueTypes{llvm::map_range(</span></a>
<a name="937"><span class="lineNum">     937 </span><span class="lineCov">        142 :       initValues.getTypes(),</span></a>
<a name="938"><span class="lineNum">     938 </span><span class="lineCov">        191 :       [](Type t) -&gt; TensorType { return t.cast&lt;TensorType&gt;(); })};</span></a>
<a name="939"><span class="lineNum">     939 </span><span class="lineCov">        142 :   uint64_t numInputs = inputs.size();</span></a>
<a name="940"><span class="lineNum">     940 </span>            : </a>
<a name="941"><span class="lineNum">     941 </span>            :   // Check for unranked tensors in input operands.</a>
<a name="942"><span class="lineNum">     942 </span><span class="lineCov">        142 :   int64_t rankedInputIdx = -1;</span></a>
<a name="943"><span class="lineNum">     943 </span><span class="lineCov">        284 :   for (uint64_t inputIdx = 0; inputIdx &lt; numInputs; ++inputIdx) {</span></a>
<a name="944"><span class="lineNum">     944 </span><span class="lineCov">        142 :     if (inputArgTypes[inputIdx].hasRank()) {</span></a>
<a name="945"><span class="lineNum">     945 </span><span class="lineCov">        140 :       rankedInputIdx = inputIdx;</span></a>
<a name="946"><span class="lineNum">     946 </span><span class="lineCov">        140 :       break;</span></a>
<a name="947"><span class="lineNum">     947 </span>            :     }</a>
<a name="948"><span class="lineNum">     948 </span><span class="lineCov">          2 :   }</span></a>
<a name="949"><span class="lineNum">     949 </span>            : </a>
<a name="950"><span class="lineNum">     950 </span><span class="lineCov">        142 :   bool allInputsUnranked = (rankedInputIdx == -1);</span></a>
<a name="951"><span class="lineNum">     951 </span>            : </a>
<a name="952"><span class="lineNum">     952 </span>            :   // P1.</a>
<a name="953"><span class="lineNum">     953 </span><span class="lineCov">        142 :   if (!allInputsUnranked) {</span></a>
<a name="954"><span class="lineNum">     954 </span><span class="lineCov">        329 :     for (uint64_t inputIdx = 0; inputIdx &lt; numInputs; ++inputIdx) {</span></a>
<a name="955"><span class="lineNum">     955 </span><span class="lineCov">        378 :       if (failed(mlir::verifyCompatibleShape(inputArgTypes[rankedInputIdx],</span></a>
<a name="956"><span class="lineNum">     956 </span><span class="lineCov">        189 :                                              inputArgTypes[inputIdx]))) {</span></a>
<a name="957"><span class="lineNum">     957 </span><span class="lineCov">          2 :         return emitOptionalError(</span></a>
<a name="958"><span class="lineNum">     958 </span><span class="lineCov">          2 :             location, &quot;expects all inputs to have compatible shapes. Shape at&quot;,</span></a>
<a name="959"><span class="lineNum">     959 </span>            :             &quot; input-index &quot;, inputIdx,</a>
<a name="960"><span class="lineNum">     960 </span>            :             &quot; is not compatible with shape at input-index &quot;, rankedInputIdx);</a>
<a name="961"><span class="lineNum">     961 </span>            :       }</a>
<a name="962"><span class="lineNum">     962 </span><span class="lineCov">        187 :     }</span></a>
<a name="963"><span class="lineNum">     963 </span><span class="lineCov">        138 :   }</span></a>
<a name="964"><span class="lineNum">     964 </span>            : </a>
<a name="965"><span class="lineNum">     965 </span>            :   // P2.</a>
<a name="966"><span class="lineNum">     966 </span><span class="lineCov">        140 :   DenseSet&lt;int64_t&gt; dimensionsToReduceSet;</span></a>
<a name="967"><span class="lineNum">     967 </span><span class="lineCov">        281 :   for (int64_t dimension : dimensions.getValues&lt;int64_t&gt;()) {</span></a>
<a name="968"><span class="lineNum">     968 </span><span class="lineCov">        418 :     if ((!allInputsUnranked &amp;&amp;</span></a>
<a name="969"><span class="lineNum">     969 </span><span class="lineCov">        139 :          dimension &gt;= inputArgTypes[rankedInputIdx].getRank()) ||</span></a>
<a name="970"><span class="lineNum">     970 </span><span class="lineCov">        140 :         dimension &lt; 0) {</span></a>
<a name="971"><span class="lineNum">     971 </span><span class="lineCov">          1 :       return emitOptionalError(</span></a>
<a name="972"><span class="lineNum">     972 </span><span class="lineCov">          1 :           location, &quot;Out-of-bounds dimension &quot;, dimension,</span></a>
<a name="973"><span class="lineNum">     973 </span><span class="lineCov">          1 :           &quot; for input-tensor rank: &quot;, inputArgTypes[rankedInputIdx].getRank());</span></a>
<a name="974"><span class="lineNum">     974 </span>            :     }</a>
<a name="975"><span class="lineNum">     975 </span>            : </a>
<a name="976"><span class="lineNum">     976 </span><span class="lineCov">        140 :     if (!dimensionsToReduceSet.insert(dimension).second) {</span></a>
<a name="977"><span class="lineNum">     977 </span><span class="lineCov">          1 :       return emitOptionalError(location,</span></a>
<a name="978"><span class="lineNum">     978 </span>            :                                &quot;Duplicate reduction dimension: &quot;, dimension);</a>
<a name="979"><span class="lineNum">     979 </span>            :     }</a>
<a name="980"><span class="lineNum">     980 </span><span class="lineCov">        141 :   }</span></a>
<a name="981"><span class="lineNum">     981 </span>            : </a>
<a name="982"><span class="lineNum">     982 </span>            :   // P3.</a>
<a name="983"><span class="lineNum">     983 </span><span class="lineCov">        138 :   SmallVector&lt;int64_t&gt; newDimensions;</span></a>
<a name="984"><span class="lineNum">     984 </span><span class="lineCov">        138 :   if (!allInputsUnranked) {</span></a>
<a name="985"><span class="lineNum">     985 </span><span class="lineCov">        405 :     for (int inputIdx = 0; inputIdx &lt; inputArgTypes[rankedInputIdx].getRank();</span></a>
<a name="986"><span class="lineNum">     986 </span><span class="lineCov">        269 :          ++inputIdx) {</span></a>
<a name="987"><span class="lineNum">     987 </span><span class="lineCov">        269 :       if (!dimensionsToReduceSet.count(inputIdx)) {</span></a>
<a name="988"><span class="lineNum">     988 </span><span class="lineCov">        266 :         newDimensions.push_back(</span></a>
<a name="989"><span class="lineNum">     989 </span><span class="lineCov">        133 :             inputArgTypes[rankedInputIdx].getDimSize(inputIdx));</span></a>
<a name="990"><span class="lineNum">     990 </span><span class="lineCov">        133 :       }</span></a>
<a name="991"><span class="lineNum">     991 </span><span class="lineCov">        269 :     }</span></a>
<a name="992"><span class="lineNum">     992 </span><span class="lineCov">        136 :   }</span></a>
<a name="993"><span class="lineNum">     993 </span>            : </a>
<a name="994"><span class="lineNum">     994 </span><span class="lineCov">        138 :   Block&amp; block = body.front();</span></a>
<a name="995"><span class="lineNum">     995 </span><span class="lineCov">        138 :   SmallVector&lt;TensorType&gt; accumulatorResultTypes;</span></a>
<a name="996"><span class="lineNum">     996 </span><span class="lineCov">        276 :   if (failed(verifyReducerShape(location, block, inputArgTypes, initValueTypes,</span></a>
<a name="997"><span class="lineNum">     997 </span><span class="lineCov">        138 :                                 numInputs, newDimensions, allInputsUnranked,</span></a>
<a name="998"><span class="lineNum">     998 </span><span class="lineCov">        138 :                                 accumulatorResultTypes)))</span></a>
<a name="999"><span class="lineNum">     999 </span><span class="lineCov">         12 :     return failure();</span></a>
<a name="1000"><span class="lineNum">    1000 </span>            : </a>
<a name="1001"><span class="lineNum">    1001 </span><span class="lineCov">        291 :   for (auto resultType : accumulatorResultTypes) {</span></a>
<a name="1002"><span class="lineNum">    1002 </span><span class="lineCov">        165 :     if (resultType.isa&lt;RankedTensorType&gt;())</span></a>
<a name="1003"><span class="lineNum">    1003 </span><span class="lineCov">        314 :       inferredReturnShapes.emplace_back(newDimensions,</span></a>
<a name="1004"><span class="lineNum">    1004 </span><span class="lineCov">        157 :                                         resultType.getElementType());</span></a>
<a name="1005"><span class="lineNum">    1005 </span>            :     else</a>
<a name="1006"><span class="lineNum">    1006 </span><span class="lineCov">          8 :       inferredReturnShapes.emplace_back(resultType.getElementType());</span></a>
<a name="1007"><span class="lineNum">    1007 </span><span class="lineCov">        165 :   }</span></a>
<a name="1008"><span class="lineNum">    1008 </span>            : </a>
<a name="1009"><span class="lineNum">    1009 </span><span class="lineCov">        126 :   return success();</span></a>
<a name="1010"><span class="lineNum">    1010 </span><span class="lineCov">        142 : }</span></a>
<a name="1011"><span class="lineNum">    1011 </span>            : </a>
<a name="1012"><span class="lineNum">    1012 </span>            : // We intend to verify the following properties</a>
<a name="1013"><span class="lineNum">    1013 </span>            : //  P2. All `inputs` need to have compatible shapes.</a>
<a name="1014"><span class="lineNum">    1014 </span>            : //  P3. size-of(window_dimension) == rank-of(input),</a>
<a name="1015"><span class="lineNum">    1015 </span>            : //        where input is an element of 'inputs'.</a>
<a name="1016"><span class="lineNum">    1016 </span>            : //  P4. Verify and collect the window atributes.</a>
<a name="1017"><span class="lineNum">    1017 </span>            : //  P5. Verify the inner block defining the reducer function.</a>
<a name="1018"><span class="lineNum">    1018 </span><span class="lineCov">         72 : LogicalResult inferReduceWindowOp(</span></a>
<a name="1019"><span class="lineNum">    1019 </span>            :     Optional&lt;Location&gt; location, ValueRange inputs, ValueRange initValues,</a>
<a name="1020"><span class="lineNum">    1020 </span>            :     DenseIntElementsAttr windowDimensions,</a>
<a name="1021"><span class="lineNum">    1021 </span>            :     Optional&lt;DenseIntElementsAttr&gt; windowStrides,</a>
<a name="1022"><span class="lineNum">    1022 </span>            :     Optional&lt;DenseIntElementsAttr&gt; baseDilations,</a>
<a name="1023"><span class="lineNum">    1023 </span>            :     Optional&lt;DenseIntElementsAttr&gt; windowDilations,</a>
<a name="1024"><span class="lineNum">    1024 </span>            :     Optional&lt;DenseIntElementsAttr&gt; padding, Region&amp; body,</a>
<a name="1025"><span class="lineNum">    1025 </span><span class="lineCov">         72 :     SmallVectorImpl&lt;ShapedTypeComponents&gt;&amp; inferredReturnShapes) {</span></a>
<a name="1026"><span class="lineNum">    1026 </span><span class="lineCov">        144 :   SmallVector&lt;TensorType&gt; inputArgTypes{llvm::map_range(</span></a>
<a name="1027"><span class="lineNum">    1027 </span><span class="lineCov">         72 :       inputs.getTypes(),</span></a>
<a name="1028"><span class="lineNum">    1028 </span><span class="lineCov">        123 :       [](Type t) -&gt; TensorType { return t.cast&lt;TensorType&gt;(); })};</span></a>
<a name="1029"><span class="lineNum">    1029 </span><span class="lineCov">        144 :   SmallVector&lt;TensorType&gt; initValueTypes{llvm::map_range(</span></a>
<a name="1030"><span class="lineNum">    1030 </span><span class="lineCov">         72 :       initValues.getTypes(),</span></a>
<a name="1031"><span class="lineNum">    1031 </span><span class="lineCov">        123 :       [](Type t) -&gt; TensorType { return t.cast&lt;TensorType&gt;(); })};</span></a>
<a name="1032"><span class="lineNum">    1032 </span><span class="lineCov">         72 :   uint64_t numInputs = inputs.size();</span></a>
<a name="1033"><span class="lineNum">    1033 </span>            : </a>
<a name="1034"><span class="lineNum">    1034 </span>            :   // Check for unranked tensors in input operands.</a>
<a name="1035"><span class="lineNum">    1035 </span><span class="lineCov">         72 :   int64_t rankedInputIdx = -1;</span></a>
<a name="1036"><span class="lineNum">    1036 </span><span class="lineCov">        148 :   for (uint64_t inputIdx = 0; inputIdx &lt; numInputs; ++inputIdx) {</span></a>
<a name="1037"><span class="lineNum">    1037 </span><span class="lineCov">         76 :     if (inputArgTypes[inputIdx].hasRank()) {</span></a>
<a name="1038"><span class="lineNum">    1038 </span><span class="lineCov">         72 :       rankedInputIdx = inputIdx;</span></a>
<a name="1039"><span class="lineNum">    1039 </span><span class="lineCov">         72 :       break;</span></a>
<a name="1040"><span class="lineNum">    1040 </span>            :     }</a>
<a name="1041"><span class="lineNum">    1041 </span><span class="lineCov">          4 :   }</span></a>
<a name="1042"><span class="lineNum">    1042 </span>            : </a>
<a name="1043"><span class="lineNum">    1043 </span><span class="lineCov">         72 :   bool allInputsUnranked = (rankedInputIdx == -1);</span></a>
<a name="1044"><span class="lineNum">    1044 </span>            : </a>
<a name="1045"><span class="lineNum">    1045 </span>            :   // P2.</a>
<a name="1046"><span class="lineNum">    1046 </span><span class="lineCov">         72 :   if (!allInputsUnranked) {</span></a>
<a name="1047"><span class="lineNum">    1047 </span><span class="lineCov">        195 :     for (uint64_t inputIdx = 0; inputIdx &lt; numInputs; ++inputIdx) {</span></a>
<a name="1048"><span class="lineNum">    1048 </span><span class="lineCov">        246 :       if (failed(mlir::verifyCompatibleShape(inputArgTypes[rankedInputIdx],</span></a>
<a name="1049"><span class="lineNum">    1049 </span><span class="lineCov">        123 :                                              inputArgTypes[inputIdx]))) {</span></a>
<a name="1050"><span class="lineNum">    1050 </span><span class="lineCov">          1 :         return emitOptionalError(</span></a>
<a name="1051"><span class="lineNum">    1051 </span><span class="lineCov">          1 :             location, &quot;expects all inputs to have compatible shapes. Shape at&quot;,</span></a>
<a name="1052"><span class="lineNum">    1052 </span>            :             &quot; input-index &quot;, inputIdx,</a>
<a name="1053"><span class="lineNum">    1053 </span>            :             &quot; is not compatible with shape at input-index &quot;, rankedInputIdx);</a>
<a name="1054"><span class="lineNum">    1054 </span>            :       }</a>
<a name="1055"><span class="lineNum">    1055 </span><span class="lineCov">        122 :     }</span></a>
<a name="1056"><span class="lineNum">    1056 </span><span class="lineCov">         71 :   }</span></a>
<a name="1057"><span class="lineNum">    1057 </span>            : </a>
<a name="1058"><span class="lineNum">    1058 </span>            :   // P3.</a>
<a name="1059"><span class="lineNum">    1059 </span><span class="lineCov">         71 :   auto windowDimsOrErr =</span></a>
<a name="1060"><span class="lineNum">    1060 </span><span class="lineCov">         71 :       convert1DAttribute(windowDimensions, location, &quot;window_dimensions&quot;);</span></a>
<a name="1061"><span class="lineNum">    1061 </span><span class="lineCov">         71 :   if (failed(windowDimsOrErr)) return failure();</span></a>
<a name="1062"><span class="lineNum">    1062 </span><span class="lineCov">        185 :   for (const auto inputType : inputArgTypes) {</span></a>
<a name="1063"><span class="lineNum">    1063 </span><span class="lineCov">        116 :     if (!inputType.hasRank()) continue;</span></a>
<a name="1064"><span class="lineNum">    1064 </span><span class="lineCov">        112 :     if (inputType.getRank() != static_cast&lt;int64_t&gt;((*windowDimsOrErr).size()))</span></a>
<a name="1065"><span class="lineNum">    1065 </span><span class="lineCov">          1 :       return emitOptionalError(</span></a>
<a name="1066"><span class="lineNum">    1066 </span><span class="lineCov">          1 :           location, &quot;expects window-dimensions size == input rank, but got &quot;,</span></a>
<a name="1067"><span class="lineNum">    1067 </span><span class="lineCov">          1 :           &quot;window-dimensions size: &quot;, (*windowDimsOrErr).size(),</span></a>
<a name="1068"><span class="lineNum">    1068 </span><span class="lineCov">          1 :           &quot; and input: &quot;, inputType, &quot; with rank = &quot;, inputType.getRank(), &quot;.&quot;);</span></a>
<a name="1069"><span class="lineNum">    1069 </span><span class="lineCov">        116 :   }</span></a>
<a name="1070"><span class="lineNum">    1070 </span>            : </a>
<a name="1071"><span class="lineNum">    1071 </span>            :   // P4.</a>
<a name="1072"><span class="lineNum">    1072 </span><span class="lineCov">         68 :   auto paddingOrErr = convertPaddingAttribute(padding, location);</span></a>
<a name="1073"><span class="lineNum">    1073 </span><span class="lineCov">         68 :   if (failed(paddingOrErr)) return failure();</span></a>
<a name="1074"><span class="lineNum">    1074 </span>            : </a>
<a name="1075"><span class="lineNum">    1075 </span><span class="lineCov">         67 :   auto windowStridesOrErr =</span></a>
<a name="1076"><span class="lineNum">    1076 </span><span class="lineCov">         67 :       convert1DAttribute(windowStrides, location, &quot;window_strides&quot;);</span></a>
<a name="1077"><span class="lineNum">    1077 </span><span class="lineCov">         67 :   if (failed(windowStridesOrErr)) return failure();</span></a>
<a name="1078"><span class="lineNum">    1078 </span><span class="lineCov">         66 :   auto baseDilationsOrErr =</span></a>
<a name="1079"><span class="lineNum">    1079 </span><span class="lineCov">         66 :       convert1DAttribute(baseDilations, location, &quot;base_dilations&quot;);</span></a>
<a name="1080"><span class="lineNum">    1080 </span><span class="lineCov">         66 :   if (failed(baseDilationsOrErr)) return failure();</span></a>
<a name="1081"><span class="lineNum">    1081 </span><span class="lineCov">         65 :   auto windowDilationsOrErr =</span></a>
<a name="1082"><span class="lineNum">    1082 </span><span class="lineCov">         65 :       convert1DAttribute(windowDilations, location, &quot;window_dilations&quot;);</span></a>
<a name="1083"><span class="lineNum">    1083 </span><span class="lineCov">         65 :   if (failed(windowDilationsOrErr)) return failure();</span></a>
<a name="1084"><span class="lineNum">    1084 </span><span class="lineCov">        128 :   auto windowOrErr = verifyWindowAttributesAndInferWindowDimensions(</span></a>
<a name="1085"><span class="lineNum">    1085 </span><span class="lineCov">         64 :       *windowDimsOrErr, *windowStridesOrErr, *paddingOrErr,</span></a>
<a name="1086"><span class="lineNum">    1086 </span><span class="lineCov">         64 :       /*lhsDilation=*/*baseDilationsOrErr,</span></a>
<a name="1087"><span class="lineNum">    1087 </span><span class="lineCov">         64 :       /*rhsDilation=*/*windowDilationsOrErr, location);</span></a>
<a name="1088"><span class="lineNum">    1088 </span><span class="lineCov">         64 :   if (failed(windowOrErr)) return failure();</span></a>
<a name="1089"><span class="lineNum">    1089 </span>            : </a>
<a name="1090"><span class="lineNum">    1090 </span>            :   // P5.</a>
<a name="1091"><span class="lineNum">    1091 </span><span class="lineCov">         56 :   Block&amp; block = body.front();</span></a>
<a name="1092"><span class="lineNum">    1092 </span><span class="lineCov">         56 :   SmallVector&lt;TensorType&gt; accumulatorSubshapes;</span></a>
<a name="1093"><span class="lineNum">    1093 </span><span class="lineCov">        112 :   if (failed(verifyReducerShape(location, block, inputArgTypes, initValueTypes,</span></a>
<a name="1094"><span class="lineNum">    1094 </span><span class="lineCov">         56 :                                 numInputs, *windowDimsOrErr, allInputsUnranked,</span></a>
<a name="1095"><span class="lineNum">    1095 </span><span class="lineCov">         56 :                                 accumulatorSubshapes)))</span></a>
<a name="1096"><span class="lineNum">    1096 </span><span class="lineCov">         12 :     return failure();</span></a>
<a name="1097"><span class="lineNum">    1097 </span>            : </a>
<a name="1098"><span class="lineNum">    1098 </span><span class="lineCov">        113 :   for (size_t i = 0; i &lt; inputArgTypes.size(); ++i) {</span></a>
<a name="1099"><span class="lineNum">    1099 </span><span class="lineCov">         69 :     if (!inputArgTypes[i].hasRank())</span></a>
<a name="1100"><span class="lineNum">    1100 </span><span class="lineCov">          2 :       inferredReturnShapes.emplace_back(initValueTypes[i].getElementType());</span></a>
<a name="1101"><span class="lineNum">    1101 </span>            :     else</a>
<a name="1102"><span class="lineNum">    1102 </span><span class="lineCov">        134 :       inferredReturnShapes.emplace_back(</span></a>
<a name="1103"><span class="lineNum">    1103 </span><span class="lineCov">         67 :           inferWindowOutputShape(inputArgTypes[i].getShape(), *windowOrErr),</span></a>
<a name="1104"><span class="lineNum">    1104 </span><span class="lineCov">         67 :           initValueTypes[i].getElementType());</span></a>
<a name="1105"><span class="lineNum">    1105 </span><span class="lineCov">         69 :   }</span></a>
<a name="1106"><span class="lineNum">    1106 </span>            : </a>
<a name="1107"><span class="lineNum">    1107 </span><span class="lineCov">         44 :   return success();</span></a>
<a name="1108"><span class="lineNum">    1108 </span><span class="lineCov">         72 : }</span></a>
<a name="1109"><span class="lineNum">    1109 </span>            : </a>
<a name="1110"><span class="lineNum">    1110 </span>            : // The following properties are already enforced by the ODS:</a>
<a name="1111"><span class="lineNum">    1111 </span>            : //  type(start_indices) == type(limit_indices) == type(strides).</a>
<a name="1112"><span class="lineNum">    1112 </span>            : // Verify the following properties:</a>
<a name="1113"><span class="lineNum">    1113 </span>            : //  P1. Verify rank(start_indices) == 1.</a>
<a name="1114"><span class="lineNum">    1114 </span>            : //  P2. Verify size(start_indices) == rank(operand).</a>
<a name="1115"><span class="lineNum">    1115 </span>            : //  P3~5. Verify 0 &lt;= start_indices[i] &lt;= limit_indices[i] &lt;= shape(operand)[i].</a>
<a name="1116"><span class="lineNum">    1116 </span>            : //  P6. Verify stride[i] &gt; 0.</a>
<a name="1117"><span class="lineNum">    1117 </span>            : // Note: for P4, use the bound size than dim size for bounded dynamism case.</a>
<a name="1118"><span class="lineNum">    1118 </span><span class="lineCov">         49 : LogicalResult inferSliceOp(Optional&lt;Location&gt; location, Value operand,</span></a>
<a name="1119"><span class="lineNum">    1119 </span>            :                            DenseIntElementsAttr startIndices,</a>
<a name="1120"><span class="lineNum">    1120 </span>            :                            DenseIntElementsAttr limitIndices,</a>
<a name="1121"><span class="lineNum">    1121 </span>            :                            DenseIntElementsAttr strides,</a>
<a name="1122"><span class="lineNum">    1122 </span><span class="lineCov">         49 :                            SmallVectorImpl&lt;Type&gt;&amp; inferredReturnTypes) {</span></a>
<a name="1123"><span class="lineNum">    1123 </span><span class="lineCov">         49 :   Type ty = operand.getType();</span></a>
<a name="1124"><span class="lineNum">    1124 </span><span class="lineCov">         49 :   RankedTensorType rankedTy = ty.dyn_cast&lt;RankedTensorType&gt;();</span></a>
<a name="1125"><span class="lineNum">    1125 </span><span class="lineCov">         49 :   if (!rankedTy) {</span></a>
<a name="1126"><span class="lineNum">    1126 </span>            :     // The operand type is unranked, so the best we can infer for the result</a>
<a name="1127"><span class="lineNum">    1127 </span>            :     // type is an unranked tensor with the same element type as the operand</a>
<a name="1128"><span class="lineNum">    1128 </span>            :     // type.</a>
<a name="1129"><span class="lineNum">    1129 </span><span class="lineCov">          2 :     inferredReturnTypes.assign({ty});</span></a>
<a name="1130"><span class="lineNum">    1130 </span><span class="lineCov">          2 :     return success();</span></a>
<a name="1131"><span class="lineNum">    1131 </span>            :   }</a>
<a name="1132"><span class="lineNum">    1132 </span>            : </a>
<a name="1133"><span class="lineNum">    1133 </span><span class="lineCov">         47 :   ShapedType attrTy = startIndices.getType();</span></a>
<a name="1134"><span class="lineNum">    1134 </span>            :   // P1.</a>
<a name="1135"><span class="lineNum">    1135 </span>            :   // Note: ODS has type(start_indices) == type(limit_indices) == type(strides)</a>
<a name="1136"><span class="lineNum">    1136 </span>            :   // So this implies rank(limit_indices) == rank(strides) == 1 also.</a>
<a name="1137"><span class="lineNum">    1137 </span><span class="lineCov">         47 :   if (attrTy.getRank() != 1) {</span></a>
<a name="1138"><span class="lineNum">    1138 </span><span class="lineCov">          2 :     return emitOptionalError(location, &quot;start_indices has rank &quot;,</span></a>
<a name="1139"><span class="lineNum">    1139 </span><span class="lineCov">          1 :                              attrTy.getRank(), &quot; instead of required rank 1&quot;);</span></a>
<a name="1140"><span class="lineNum">    1140 </span>            :   }</a>
<a name="1141"><span class="lineNum">    1141 </span>            : </a>
<a name="1142"><span class="lineNum">    1142 </span>            :   // P2.</a>
<a name="1143"><span class="lineNum">    1143 </span><span class="lineCov">         46 :   int64_t rank = rankedTy.getRank();</span></a>
<a name="1144"><span class="lineNum">    1144 </span><span class="lineCov">         46 :   if (attrTy.getNumElements() != rank) {</span></a>
<a name="1145"><span class="lineNum">    1145 </span><span class="lineCov">          1 :     return emitOptionalError(</span></a>
<a name="1146"><span class="lineNum">    1146 </span><span class="lineCov">          1 :         location, &quot;the number of elements in start_indices (&quot;,</span></a>
<a name="1147"><span class="lineNum">    1147 </span><span class="lineCov">          1 :         attrTy.getNumElements(), &quot;) does not match the rank of the operand (&quot;,</span></a>
<a name="1148"><span class="lineNum">    1148 </span>            :         rank, &quot;)&quot;);</a>
<a name="1149"><span class="lineNum">    1149 </span>            :   }</a>
<a name="1150"><span class="lineNum">    1150 </span>            : </a>
<a name="1151"><span class="lineNum">    1151 </span><span class="lineCov">         45 :   SmallVector&lt;int64_t, 4&gt; start(startIndices.getValues&lt;int64_t&gt;());</span></a>
<a name="1152"><span class="lineNum">    1152 </span><span class="lineCov">         45 :   SmallVector&lt;int64_t, 4&gt; limit(limitIndices.getValues&lt;int64_t&gt;());</span></a>
<a name="1153"><span class="lineNum">    1153 </span><span class="lineCov">         45 :   SmallVector&lt;int64_t, 4&gt; strideVals(strides.getValues&lt;int64_t&gt;());</span></a>
<a name="1154"><span class="lineNum">    1154 </span>            : </a>
<a name="1155"><span class="lineNum">    1155 </span><span class="lineCov">         45 :   ArrayRef&lt;int64_t&gt; inputBounds = encodingToBounds(rankedTy.getEncoding());</span></a>
<a name="1156"><span class="lineNum">    1156 </span><span class="lineCov">         45 :   SmallVector&lt;int64_t&gt; shape(rank, ShapedType::kDynamicSize);</span></a>
<a name="1157"><span class="lineNum">    1157 </span><span class="lineCov">         45 :   SmallVector&lt;int64_t&gt; resultBounds(inputBounds.size(),</span></a>
<a name="1158"><span class="lineNum">    1158 </span>            :                                     ShapedType::kDynamicSize);</a>
<a name="1159"><span class="lineNum">    1159 </span>            : </a>
<a name="1160"><span class="lineNum">    1160 </span><span class="lineCov">        122 :   for (int64_t i = 0, e = rank; i != e; i++) {</span></a>
<a name="1161"><span class="lineNum">    1161 </span>            :     // P3.</a>
<a name="1162"><span class="lineNum">    1162 </span><span class="lineCov">         77 :     if (start[i] &lt; 0)</span></a>
<a name="1163"><span class="lineNum">    1163 </span><span class="lineCov">          2 :       return emitOptionalError(location, &quot;negative start index &quot;, start[i],</span></a>
<a name="1164"><span class="lineNum">    1164 </span>            :                                &quot; in dimension &quot;, i);</a>
<a name="1165"><span class="lineNum">    1165 </span>            : </a>
<a name="1166"><span class="lineNum">    1166 </span>            :     // P4.</a>
<a name="1167"><span class="lineNum">    1167 </span><span class="lineCov">         75 :     bool isStaticDim = !hlo::isDynamicDimSize(rankedTy.getDimSize(i));</span></a>
<a name="1168"><span class="lineNum">    1168 </span><span class="lineCov">         89 :     bool isStaticBound =</span></a>
<a name="1169"><span class="lineNum">    1169 </span><span class="lineCov">         75 :         !inputBounds.empty() &amp;&amp; !hlo::isDynamicDimSize(inputBounds[i]);</span></a>
<a name="1170"><span class="lineNum">    1170 </span><span class="lineCov">         75 :     if (isStaticDim || isStaticBound) {</span></a>
<a name="1171"><span class="lineNum">    1171 </span><span class="lineCov">        138 :       int64_t operandSizeOrBound =</span></a>
<a name="1172"><span class="lineNum">    1172 </span><span class="lineCov">         69 :           isStaticDim ? rankedTy.getDimSize(i) : inputBounds[i];</span></a>
<a name="1173"><span class="lineNum">    1173 </span><span class="lineCov">         69 :       StringRef sizeOrBound = isStaticDim ? &quot;size&quot; : &quot;bound&quot;;</span></a>
<a name="1174"><span class="lineNum">    1174 </span><span class="lineCov">         69 :       if (limit[i] &gt; operandSizeOrBound)</span></a>
<a name="1175"><span class="lineNum">    1175 </span><span class="lineCov">          2 :         return emitOptionalError(location, &quot;limit index &quot;, limit[i],</span></a>
<a name="1176"><span class="lineNum">    1176 </span>            :                                  &quot; is larger than dimension &quot;, sizeOrBound, &quot; &quot;,</a>
<a name="1177"><span class="lineNum">    1177 </span>            :                                  operandSizeOrBound, &quot; in dimension &quot;, i);</a>
<a name="1178"><span class="lineNum">    1178 </span><span class="lineCov">         69 :     }</span></a>
<a name="1179"><span class="lineNum">    1179 </span>            : </a>
<a name="1180"><span class="lineNum">    1180 </span>            :     // P5.</a>
<a name="1181"><span class="lineNum">    1181 </span><span class="lineCov">         73 :     if (start[i] &gt; limit[i])</span></a>
<a name="1182"><span class="lineNum">    1182 </span><span class="lineCov">          2 :       return emitOptionalError(location, &quot;start index &quot;, start[i],</span></a>
<a name="1183"><span class="lineNum">    1183 </span><span class="lineCov">          1 :                                &quot; is larger than limit index &quot;, limit[i],</span></a>
<a name="1184"><span class="lineNum">    1184 </span>            :                                &quot; in dimension &quot;, i);</a>
<a name="1185"><span class="lineNum">    1185 </span>            :     // P6.</a>
<a name="1186"><span class="lineNum">    1186 </span><span class="lineCov">         72 :     if (strideVals[i] &lt;= 0)</span></a>
<a name="1187"><span class="lineNum">    1187 </span><span class="lineCov">          2 :       return emitOptionalError(location, &quot;stride must be positive but got &quot;,</span></a>
<a name="1188"><span class="lineNum">    1188 </span><span class="lineCov">          1 :                                strideVals[i], &quot; in dimension &quot;, i);</span></a>
<a name="1189"><span class="lineNum">    1189 </span>            : </a>
<a name="1190"><span class="lineNum">    1190 </span><span class="lineCov">         71 :     shape[i] = static_cast&lt;int64_t&gt;(</span></a>
<a name="1191"><span class="lineNum">    1191 </span><span class="lineCov">         71 :         llvm::divideCeil(limit[i] - start[i], strideVals[i]));</span></a>
<a name="1192"><span class="lineNum">    1192 </span><span class="lineCov">         75 :   }</span></a>
<a name="1193"><span class="lineNum">    1193 </span>            : </a>
<a name="1194"><span class="lineNum">    1194 </span><span class="lineCov">         78 :   inferredReturnTypes.push_back(RankedTensorType::get(</span></a>
<a name="1195"><span class="lineNum">    1195 </span><span class="lineCov">         39 :       shape, rankedTy.getElementType(),</span></a>
<a name="1196"><span class="lineNum">    1196 </span><span class="lineCov">         39 :       boundsToEncoding(rankedTy.getEncoding(), resultBounds)));</span></a>
<a name="1197"><span class="lineNum">    1197 </span><span class="lineCov">         39 :   return success();</span></a>
<a name="1198"><span class="lineNum">    1198 </span><span class="lineCov">         49 : }</span></a>
<a name="1199"><span class="lineNum">    1199 </span>            : </a>
<a name="1200"><span class="lineNum">    1200 </span><span class="lineCov">         47 : LogicalResult inferSortOp(</span></a>
<a name="1201"><span class="lineNum">    1201 </span>            :     Optional&lt;Location&gt; location, ValueRange inputs, uint64_t dimension,</a>
<a name="1202"><span class="lineNum">    1202 </span>            :     Region&amp; comparator,</a>
<a name="1203"><span class="lineNum">    1203 </span><span class="lineCov">         47 :     SmallVectorImpl&lt;ShapedTypeComponents&gt;&amp; inferredReturnShapes) {</span></a>
<a name="1204"><span class="lineNum">    1204 </span><span class="lineCov">         47 :   auto operandTypes = inputs.getTypes();</span></a>
<a name="1205"><span class="lineNum">    1205 </span><span class="lineCov">         97 :   for (auto operandType : operandTypes) {</span></a>
<a name="1206"><span class="lineNum">    1206 </span><span class="lineCov">         50 :     auto operandShapedType = operandType.cast&lt;ShapedType&gt;();</span></a>
<a name="1207"><span class="lineNum">    1207 </span><span class="lineCov">         50 :     if (operandShapedType.hasRank()) {</span></a>
<a name="1208"><span class="lineNum">    1208 </span><span class="lineCov">         47 :       int64_t cmpDim = dimension;</span></a>
<a name="1209"><span class="lineNum">    1209 </span><span class="lineCov">         47 :       int64_t rank = operandShapedType.getRank();</span></a>
<a name="1210"><span class="lineNum">    1210 </span><span class="lineCov">         47 :       if (cmpDim &lt; -rank || cmpDim &gt;= rank)</span></a>
<a name="1211"><span class="lineNum">    1211 </span><span class="lineCov">          2 :         return emitOptionalError(</span></a>
<a name="1212"><span class="lineNum">    1212 </span><span class="lineCov">          2 :             location, &quot;dimension attribute value must be in range [-&quot;, rank,</span></a>
<a name="1213"><span class="lineNum">    1213 </span>            :             &quot;, &quot;, rank, &quot;), but found &quot;, cmpDim);</a>
<a name="1214"><span class="lineNum">    1214 </span>            :       else</a>
<a name="1215"><span class="lineNum">    1215 </span><span class="lineCov">         45 :         break;  // ODS SameOperandsAndResultShape asserts inputs have same shape</span></a>
<a name="1216"><span class="lineNum">    1216 </span><span class="lineCov">         47 :     }</span></a>
<a name="1217"><span class="lineNum">    1217 </span><span class="lineCov">         50 :   }</span></a>
<a name="1218"><span class="lineNum">    1218 </span>            : </a>
<a name="1219"><span class="lineNum">    1219 </span>            :   // Comparator must have 2 * N scalar arguments of same type as the N inputs.</a>
<a name="1220"><span class="lineNum">    1220 </span><span class="lineCov">         45 :   Block&amp; block = comparator.front();</span></a>
<a name="1221"><span class="lineNum">    1221 </span><span class="lineCov">         45 :   size_t numOperands = operandTypes.size();</span></a>
<a name="1222"><span class="lineNum">    1222 </span><span class="lineCov">         45 :   if (block.getNumArguments() != 2 * numOperands)</span></a>
<a name="1223"><span class="lineNum">    1223 </span><span class="lineCov">          2 :     return emitOptionalError(location, &quot;comparator block should have &quot;,</span></a>
<a name="1224"><span class="lineNum">    1224 </span><span class="lineCov">          1 :                              2 * numOperands, &quot; arguments&quot;);</span></a>
<a name="1225"><span class="lineNum">    1225 </span><span class="lineCov">        118 :   for (const auto&amp; indexedOperandType : llvm::enumerate(operandTypes)) {</span></a>
<a name="1226"><span class="lineNum">    1226 </span><span class="lineCov">         74 :     int index = indexedOperandType.index();</span></a>
<a name="1227"><span class="lineNum">    1227 </span><span class="lineCov">         74 :     Type elementType =</span></a>
<a name="1228"><span class="lineNum">    1228 </span><span class="lineCov">         74 :         indexedOperandType.value().cast&lt;ShapedType&gt;().getElementType();</span></a>
<a name="1229"><span class="lineNum">    1229 </span><span class="lineCov">         74 :     Type tensorType = RankedTensorType::get({}, elementType);</span></a>
<a name="1230"><span class="lineNum">    1230 </span><span class="lineCov">        221 :     for (int i : {2 * index, 2 * index + 1}) {</span></a>
<a name="1231"><span class="lineNum">    1231 </span><span class="lineCov">        147 :       Type argType = block.getArgument(i).getType();</span></a>
<a name="1232"><span class="lineNum">    1232 </span><span class="lineCov">        147 :       if (argType != tensorType)</span></a>
<a name="1233"><span class="lineNum">    1233 </span><span class="lineCov">          2 :         return emitOptionalError(location, &quot;comparator block argument #&quot;, i,</span></a>
<a name="1234"><span class="lineNum">    1234 </span>            :                                  &quot; should be of type &quot;, tensorType, &quot; but got &quot;,</a>
<a name="1235"><span class="lineNum">    1235 </span>            :                                  argType);</a>
<a name="1236"><span class="lineNum">    1236 </span><span class="lineCov">        147 :     }</span></a>
<a name="1237"><span class="lineNum">    1237 </span><span class="lineCov">         74 :   }</span></a>
<a name="1238"><span class="lineNum">    1238 </span>            : </a>
<a name="1239"><span class="lineNum">    1239 </span>            :   // Comparator must return single 0-ranked tensor with element-type i1.</a>
<a name="1240"><span class="lineNum">    1240 </span><span class="lineCov">         42 :   auto comparatorResult = block.getTerminator()-&gt;getOperands();</span></a>
<a name="1241"><span class="lineNum">    1241 </span><span class="lineCov">         42 :   if (comparatorResult.size() != 1)</span></a>
<a name="1242"><span class="lineNum">    1242 </span><span class="lineCov">          2 :     return emitOptionalError(location,</span></a>
<a name="1243"><span class="lineNum">    1243 </span>            :                              &quot;comparator must return single output but got &quot;,</a>
<a name="1244"><span class="lineNum">    1244 </span><span class="lineCov">          1 :                              comparatorResult.size());</span></a>
<a name="1245"><span class="lineNum">    1245 </span><span class="lineCov">         41 :   auto comparatorResultType = comparatorResult[0].getType().cast&lt;TensorType&gt;();</span></a>
<a name="1246"><span class="lineNum">    1246 </span><span class="lineCov">         41 :   if ((comparatorResultType.hasRank() &amp;&amp; comparatorResultType.getRank() != 0) ||</span></a>
<a name="1247"><span class="lineNum">    1247 </span><span class="lineCov">         40 :       !comparatorResultType.getElementType().isInteger(1))</span></a>
<a name="1248"><span class="lineNum">    1248 </span><span class="lineCov">          4 :     return emitOptionalError(location,</span></a>
<a name="1249"><span class="lineNum">    1249 </span>            :                              &quot;comparator must return tensor&lt;i1&gt; but got &quot;,</a>
<a name="1250"><span class="lineNum">    1250 </span><span class="lineCov">          2 :                              comparatorResult[0].getType());</span></a>
<a name="1251"><span class="lineNum">    1251 </span>            : </a>
<a name="1252"><span class="lineNum">    1252 </span><span class="lineCov">        104 :   for (auto resultType : operandTypes)</span></a>
<a name="1253"><span class="lineNum">    1253 </span><span class="lineCov">         65 :     inferredReturnShapes.emplace_back(resultType.cast&lt;ShapedType&gt;());</span></a>
<a name="1254"><span class="lineNum">    1254 </span><span class="lineCov">         39 :   return success();</span></a>
<a name="1255"><span class="lineNum">    1255 </span><span class="lineCov">         47 : }</span></a>
<a name="1256"><span class="lineNum">    1256 </span>            : </a>
<a name="1257"><span class="lineNum">    1257 </span><span class="lineCov">         48 : LogicalResult inferTransposeOp(Optional&lt;Location&gt; loc, Value operand,</span></a>
<a name="1258"><span class="lineNum">    1258 </span>            :                                DenseIntElementsAttr permutation,</a>
<a name="1259"><span class="lineNum">    1259 </span><span class="lineCov">         48 :                                SmallVectorImpl&lt;Type&gt;&amp; inferredReturnTypes) {</span></a>
<a name="1260"><span class="lineNum">    1260 </span><span class="lineCov">         48 :   auto type = operand.getType();</span></a>
<a name="1261"><span class="lineNum">    1261 </span><span class="lineCov">         48 :   auto rankedTy = type.dyn_cast&lt;RankedTensorType&gt;();</span></a>
<a name="1262"><span class="lineNum">    1262 </span><span class="lineCov">         48 :   if (!rankedTy) {</span></a>
<a name="1263"><span class="lineNum">    1263 </span><span class="lineCov">          2 :     inferredReturnTypes.emplace_back(type);</span></a>
<a name="1264"><span class="lineNum">    1264 </span><span class="lineCov">          2 :     return success();</span></a>
<a name="1265"><span class="lineNum">    1265 </span>            :   }</a>
<a name="1266"><span class="lineNum">    1266 </span><span class="lineCov">         46 :   int64_t rank = rankedTy.getRank();</span></a>
<a name="1267"><span class="lineNum">    1267 </span><span class="lineCov">         46 :   if (permutation.getType().getRank() != 1)</span></a>
<a name="1268"><span class="lineNum">    1268 </span><span class="lineCov">          2 :     return emitOptionalError(loc, &quot;TransposeOp permutation has rank &quot;,</span></a>
<a name="1269"><span class="lineNum">    1269 </span><span class="lineCov">          1 :                              permutation.getType().getRank(),</span></a>
<a name="1270"><span class="lineNum">    1270 </span>            :                              &quot; instead of rank 1&quot;);</a>
<a name="1271"><span class="lineNum">    1271 </span>            : </a>
<a name="1272"><span class="lineNum">    1272 </span><span class="lineCov">         45 :   if (permutation.size() != rank)</span></a>
<a name="1273"><span class="lineNum">    1273 </span><span class="lineCov">          2 :     return emitOptionalError(loc, &quot;TransposeOp operand rank &quot;, rank,</span></a>
<a name="1274"><span class="lineNum">    1274 </span>            :                              &quot; does not match permutation size &quot;,</a>
<a name="1275"><span class="lineNum">    1275 </span><span class="lineCov">          1 :                              permutation.size());</span></a>
<a name="1276"><span class="lineNum">    1276 </span>            : </a>
<a name="1277"><span class="lineNum">    1277 </span><span class="lineCov">         44 :   std::vector&lt;int64_t&gt; range(rank);</span></a>
<a name="1278"><span class="lineNum">    1278 </span><span class="lineCov">         44 :   std::iota(range.begin(), range.end(), 0);</span></a>
<a name="1279"><span class="lineNum">    1279 </span><span class="lineCov">         44 :   if (!std::is_permutation(range.begin(), range.end(), permutation.begin()))</span></a>
<a name="1280"><span class="lineNum">    1280 </span><span class="lineCov">          1 :     return emitOptionalError(loc,</span></a>
<a name="1281"><span class="lineNum">    1281 </span>            :                              &quot;attribute permutation must be a permutation&quot;</a>
<a name="1282"><span class="lineNum">    1282 </span>            :                              &quot; of [&quot;,</a>
<a name="1283"><span class="lineNum">    1283 </span>            :                              range, &quot;] but got &quot;, permutation);</a>
<a name="1284"><span class="lineNum">    1284 </span>            : </a>
<a name="1285"><span class="lineNum">    1285 </span><span class="lineCov">         43 :   ArrayRef&lt;int64_t&gt; inputBounds = encodingToBounds(rankedTy.getEncoding());</span></a>
<a name="1286"><span class="lineNum">    1286 </span><span class="lineCov">         43 :   SmallVector&lt;int64_t&gt; resultShape;</span></a>
<a name="1287"><span class="lineNum">    1287 </span><span class="lineCov">         43 :   SmallVector&lt;int64_t&gt; resultBounds;</span></a>
<a name="1288"><span class="lineNum">    1288 </span><span class="lineCov">         43 :   ArrayRef&lt;int64_t&gt; inputShape = rankedTy.getShape();</span></a>
<a name="1289"><span class="lineNum">    1289 </span><span class="lineCov">        191 :   for (int64_t dim : permutation.getValues&lt;int64_t&gt;()) {</span></a>
<a name="1290"><span class="lineNum">    1290 </span><span class="lineCov">        148 :     resultShape.push_back(inputShape[dim]);</span></a>
<a name="1291"><span class="lineNum">    1291 </span><span class="lineCov">        148 :     if (!inputBounds.empty()) {</span></a>
<a name="1292"><span class="lineNum">    1292 </span><span class="lineCov">         16 :       resultBounds.push_back(inputBounds[dim]);</span></a>
<a name="1293"><span class="lineNum">    1293 </span><span class="lineCov">         16 :     }</span></a>
<a name="1294"><span class="lineNum">    1294 </span><span class="lineCov">        148 :   }</span></a>
<a name="1295"><span class="lineNum">    1295 </span>            : </a>
<a name="1296"><span class="lineNum">    1296 </span><span class="lineCov">         86 :   inferredReturnTypes.push_back(RankedTensorType::get(</span></a>
<a name="1297"><span class="lineNum">    1297 </span><span class="lineCov">         43 :       resultShape, rankedTy.getElementType(),</span></a>
<a name="1298"><span class="lineNum">    1298 </span><span class="lineCov">         43 :       boundsToEncoding(rankedTy.getEncoding(), resultBounds)));</span></a>
<a name="1299"><span class="lineNum">    1299 </span><span class="lineCov">         43 :   return success();</span></a>
<a name="1300"><span class="lineNum">    1300 </span><span class="lineCov">         48 : }</span></a>
<a name="1301"><span class="lineNum">    1301 </span>            : </a>
<a name="1302"><span class="lineNum">    1302 </span><span class="lineCov">         45 : LogicalResult inferTriangularSolveOp(</span></a>
<a name="1303"><span class="lineNum">    1303 </span>            :     Optional&lt;Location&gt; location, Value a, Value b, bool leftSide,</a>
<a name="1304"><span class="lineNum">    1304 </span>            :     bool isTransposeAInvalid,</a>
<a name="1305"><span class="lineNum">    1305 </span><span class="lineCov">         45 :     SmallVectorImpl&lt;ShapedTypeComponents&gt;&amp; inferredReturnShapes) {</span></a>
<a name="1306"><span class="lineNum">    1306 </span>            :   // ODS enforces that a and b are of same element type: float or complex.</a>
<a name="1307"><span class="lineNum">    1307 </span><span class="lineCov">         45 :   auto elementType = a.getType().cast&lt;ShapedType&gt;().getElementType();</span></a>
<a name="1308"><span class="lineNum">    1308 </span><span class="lineCov">         45 :   auto aType = a.getType().dyn_cast&lt;RankedTensorType&gt;();</span></a>
<a name="1309"><span class="lineNum">    1309 </span><span class="lineCov">         45 :   if (!aType) {</span></a>
<a name="1310"><span class="lineNum">    1310 </span><span class="lineCov">          4 :     inferredReturnShapes.emplace_back(elementType);</span></a>
<a name="1311"><span class="lineNum">    1311 </span><span class="lineCov">          4 :     return success();</span></a>
<a name="1312"><span class="lineNum">    1312 </span>            :   }</a>
<a name="1313"><span class="lineNum">    1313 </span>            : </a>
<a name="1314"><span class="lineNum">    1314 </span><span class="lineCov">         41 :   auto aRank = aType.getRank();</span></a>
<a name="1315"><span class="lineNum">    1315 </span><span class="lineCov">         41 :   if (aRank &lt; 2)</span></a>
<a name="1316"><span class="lineNum">    1316 </span><span class="lineCov">          1 :     return emitOptionalError(</span></a>
<a name="1317"><span class="lineNum">    1317 </span><span class="lineCov">          1 :         location, &quot;operand 'a' must have rank &gt;= 2, but got &quot;, aType);</span></a>
<a name="1318"><span class="lineNum">    1318 </span>            : </a>
<a name="1319"><span class="lineNum">    1319 </span><span class="lineCov">         40 :   if (aType.getDimSize(aRank - 2) != aType.getDimSize(aRank - 1))</span></a>
<a name="1320"><span class="lineNum">    1320 </span><span class="lineCov">          1 :     return emitOptionalError(location,</span></a>
<a name="1321"><span class="lineNum">    1321 </span>            :                              &quot;two minor dimensions of operand 'a' must have &quot;</a>
<a name="1322"><span class="lineNum">    1322 </span>            :                              &quot;equal size, but got &quot;,</a>
<a name="1323"><span class="lineNum">    1323 </span>            :                              aType);</a>
<a name="1324"><span class="lineNum">    1324 </span>            : </a>
<a name="1325"><span class="lineNum">    1325 </span><span class="lineCov">         39 :   auto bType = b.getType().dyn_cast&lt;RankedTensorType&gt;();</span></a>
<a name="1326"><span class="lineNum">    1326 </span><span class="lineCov">         39 :   if (!bType) {</span></a>
<a name="1327"><span class="lineNum">    1327 </span><span class="lineCov">          2 :     inferredReturnShapes.emplace_back(elementType);</span></a>
<a name="1328"><span class="lineNum">    1328 </span><span class="lineCov">          2 :     return success();</span></a>
<a name="1329"><span class="lineNum">    1329 </span>            :   }</a>
<a name="1330"><span class="lineNum">    1330 </span>            : </a>
<a name="1331"><span class="lineNum">    1331 </span><span class="lineCov">         37 :   auto bRank = bType.getRank();</span></a>
<a name="1332"><span class="lineNum">    1332 </span><span class="lineCov">         37 :   if (aRank != bRank)</span></a>
<a name="1333"><span class="lineNum">    1333 </span><span class="lineCov">          1 :     return emitOptionalError(location,</span></a>
<a name="1334"><span class="lineNum">    1334 </span>            :                              &quot;operands must have equal rank, but got &quot;, aType,</a>
<a name="1335"><span class="lineNum">    1335 </span>            :                              &quot; and &quot;, bType);</a>
<a name="1336"><span class="lineNum">    1336 </span>            : </a>
<a name="1337"><span class="lineNum">    1337 </span>            :   // The shared dimension of a and b should match.</a>
<a name="1338"><span class="lineNum">    1338 </span><span class="lineCov">         72 :   if (aType.getDimSize(aRank - 1) !=</span></a>
<a name="1339"><span class="lineNum">    1339 </span><span class="lineCov">         36 :       bType.getDimSize(bRank - (leftSide ? 2 : 1)))</span></a>
<a name="1340"><span class="lineNum">    1340 </span><span class="lineCov">          1 :     return emitOptionalError(location,</span></a>
<a name="1341"><span class="lineNum">    1341 </span>            :                              &quot;shared dimension of operands 'a' and 'b' does &quot;</a>
<a name="1342"><span class="lineNum">    1342 </span>            :                              &quot;not match, but got &quot;,</a>
<a name="1343"><span class="lineNum">    1343 </span>            :                              aType, &quot; and &quot;, bType);</a>
<a name="1344"><span class="lineNum">    1344 </span>            : </a>
<a name="1345"><span class="lineNum">    1345 </span>            :   // The leading batch dimensions of a and b must be equal.</a>
<a name="1346"><span class="lineNum">    1346 </span><span class="lineCov">         35 :   auto aBatchDims = aType.getShape().drop_back(2);</span></a>
<a name="1347"><span class="lineNum">    1347 </span><span class="lineCov">         35 :   auto bBatchDims = bType.getShape().drop_back(2);</span></a>
<a name="1348"><span class="lineNum">    1348 </span><span class="lineCov">         35 :   if (aBatchDims != bBatchDims)</span></a>
<a name="1349"><span class="lineNum">    1349 </span><span class="lineCov">          1 :     return emitOptionalError(</span></a>
<a name="1350"><span class="lineNum">    1350 </span><span class="lineCov">          1 :         location,</span></a>
<a name="1351"><span class="lineNum">    1351 </span>            :         &quot;leading batch dimensions of the operands must be same, but got &quot;,</a>
<a name="1352"><span class="lineNum">    1352 </span>            :         aType, &quot; and &quot;, bType);</a>
<a name="1353"><span class="lineNum">    1353 </span>            : </a>
<a name="1354"><span class="lineNum">    1354 </span><span class="lineCov">         34 :   if (isTransposeAInvalid)</span></a>
<a name="1355"><span class="lineNum">    1355 </span><span class="lineCov">          1 :     return emitOptionalError(</span></a>
<a name="1356"><span class="lineNum">    1356 </span><span class="lineCov">          1 :         location, &quot;Invalid transpose option value for triangular solve&quot;);</span></a>
<a name="1357"><span class="lineNum">    1357 </span>            : </a>
<a name="1358"><span class="lineNum">    1358 </span><span class="lineCov">         33 :   inferredReturnShapes.emplace_back(bType.cast&lt;ShapedType&gt;());</span></a>
<a name="1359"><span class="lineNum">    1359 </span><span class="lineCov">         33 :   return success();</span></a>
<a name="1360"><span class="lineNum">    1360 </span><span class="lineCov">         45 : }</span></a>
<a name="1361"><span class="lineNum">    1361 </span>            : </a>
<a name="1362"><span class="lineNum">    1362 </span><span class="lineCov">         34 : LogicalResult inferWhileOp(Optional&lt;Location&gt; location, ValueRange operand,</span></a>
<a name="1363"><span class="lineNum">    1363 </span>            :                            Region&amp; cond, Region&amp; body,</a>
<a name="1364"><span class="lineNum">    1364 </span><span class="lineCov">         34 :                            SmallVectorImpl&lt;Type&gt;&amp; inferredReturnTypes) {</span></a>
<a name="1365"><span class="lineNum">    1365 </span><span class="lineCov">         34 :   auto operandTypes = operand.getTypes();</span></a>
<a name="1366"><span class="lineNum">    1366 </span><span class="lineCov">         34 :   auto condArgsTypes = cond.front().getArgumentTypes();</span></a>
<a name="1367"><span class="lineNum">    1367 </span><span class="lineCov">         34 :   auto bodyArgsTypes = body.front().getArgumentTypes();</span></a>
<a name="1368"><span class="lineNum">    1368 </span><span class="lineCov">         34 :   if (!hlo::isCompatibleForHloTypeInference(operandTypes, condArgsTypes))</span></a>
<a name="1369"><span class="lineNum">    1369 </span><span class="lineCov">          2 :     return emitOptionalError(location,</span></a>
<a name="1370"><span class="lineNum">    1370 </span>            :                              &quot;expect operands are compatible with condition &quot;</a>
<a name="1371"><span class="lineNum">    1371 </span>            :                              &quot;block arguments but got &quot;,</a>
<a name="1372"><span class="lineNum">    1372 </span>            :                              operandTypes, &quot; vs &quot;, condArgsTypes);</a>
<a name="1373"><span class="lineNum">    1373 </span><span class="lineCov">         32 :   if (!hlo::isCompatibleForHloTypeInference(operandTypes, bodyArgsTypes))</span></a>
<a name="1374"><span class="lineNum">    1374 </span><span class="lineCov">          2 :     return emitOptionalError(</span></a>
<a name="1375"><span class="lineNum">    1375 </span><span class="lineCov">          2 :         location,</span></a>
<a name="1376"><span class="lineNum">    1376 </span>            :         &quot;expect operands are compatible with body block arguments but got &quot;,</a>
<a name="1377"><span class="lineNum">    1377 </span>            :         operandTypes, &quot; vs &quot;, bodyArgsTypes);</a>
<a name="1378"><span class="lineNum">    1378 </span>            : </a>
<a name="1379"><span class="lineNum">    1379 </span><span class="lineCov">         30 :   auto bodyReturnTypes = body.front().getTerminator()-&gt;getOperandTypes();</span></a>
<a name="1380"><span class="lineNum">    1380 </span><span class="lineCov">         30 :   if (!hlo::isCompatibleForHloTypeInference(operandTypes, bodyReturnTypes))</span></a>
<a name="1381"><span class="lineNum">    1381 </span><span class="lineCov">          2 :     return emitOptionalError(</span></a>
<a name="1382"><span class="lineNum">    1382 </span><span class="lineCov">          2 :         location,</span></a>
<a name="1383"><span class="lineNum">    1383 </span>            :         &quot;expect operands are compatible with body block return types but got &quot;,</a>
<a name="1384"><span class="lineNum">    1384 </span>            :         operandTypes, &quot; vs &quot;, bodyReturnTypes);</a>
<a name="1385"><span class="lineNum">    1385 </span>            : </a>
<a name="1386"><span class="lineNum">    1386 </span><span class="lineCov">         28 :   auto condReturnTypes = cond.front().back().getOperandTypes();</span></a>
<a name="1387"><span class="lineNum">    1387 </span><span class="lineCov">         28 :   if (condReturnTypes.size() != 1)</span></a>
<a name="1388"><span class="lineNum">    1388 </span><span class="lineCov">          1 :     return emitOptionalError(</span></a>
<a name="1389"><span class="lineNum">    1389 </span><span class="lineCov">          1 :         location, &quot;expect condition body returns a single value but got &quot;,</span></a>
<a name="1390"><span class="lineNum">    1390 </span><span class="lineCov">          1 :         condReturnTypes.size());</span></a>
<a name="1391"><span class="lineNum">    1391 </span><span class="lineCov">         27 :   auto operandType = condReturnTypes[0].cast&lt;TensorType&gt;();</span></a>
<a name="1392"><span class="lineNum">    1392 </span><span class="lineCov">         27 :   if ((operandType.hasRank() &amp;&amp; operandType.getRank() != 0) ||</span></a>
<a name="1393"><span class="lineNum">    1393 </span><span class="lineCov">         26 :       !operandType.getElementType().isInteger(1))</span></a>
<a name="1394"><span class="lineNum">    1394 </span><span class="lineCov">          3 :     return emitOptionalError(</span></a>
<a name="1395"><span class="lineNum">    1395 </span><span class="lineCov">          3 :         location,</span></a>
<a name="1396"><span class="lineNum">    1396 </span>            :         &quot;expect condition block return a zero-ranked tensor of i1 but got &quot;,</a>
<a name="1397"><span class="lineNum">    1397 </span><span class="lineCov">          3 :         condReturnTypes[0]);</span></a>
<a name="1398"><span class="lineNum">    1398 </span>            : </a>
<a name="1399"><span class="lineNum">    1399 </span><span class="lineCov">         85 :   for (const auto&amp; resultType : operand.getType())</span></a>
<a name="1400"><span class="lineNum">    1400 </span><span class="lineCov">         61 :     inferredReturnTypes.push_back(resultType);</span></a>
<a name="1401"><span class="lineNum">    1401 </span><span class="lineCov">         24 :   return success();</span></a>
<a name="1402"><span class="lineNum">    1402 </span><span class="lineCov">         34 : }</span></a>
<a name="1403"><span class="lineNum">    1403 </span>            : </a>
<a name="1404"><span class="lineNum">    1404 </span>            : }  // end namespace hlo</a>
<a name="1405"><span class="lineNum">    1405 </span>            : }  // end namespace mlir</a>
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="http://ltp.sourceforge.net/coverage/lcov.php" target="_parent">LCOV version 1.14</a></td></tr>
  </table>
  <br>

</body>
</html>
